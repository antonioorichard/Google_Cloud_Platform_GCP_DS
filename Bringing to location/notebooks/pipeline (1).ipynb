{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e82fe76d-d82e-4f34-813b-5ed9e517fb81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: kfp 1.8.22\n",
      "Uninstalling kfp-1.8.22:\n",
      "  Successfully uninstalled kfp-1.8.22\n",
      "\u001b[33mWARNING: Skipping google-cloud-pipeline-components as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall kfp -y\n",
    "!pip uninstall google-cloud-pipeline-components -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74ff06ba-0568-4889-8c65-081b31c16407",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: kfp\n",
      "Version: 1.8.22\n",
      "Summary: KubeFlow Pipelines SDK\n",
      "Home-page: https://github.com/kubeflow/pipelines\n",
      "Author: The Kubeflow Authors\n",
      "Author-email: \n",
      "License: \n",
      "Location: /opt/conda/lib/python3.10/site-packages\n",
      "Requires: absl-py, click, cloudpickle, Deprecated, docstring-parser, fire, google-api-core, google-api-python-client, google-auth, google-cloud-storage, jsonschema, kfp-pipeline-spec, kfp-server-api, kubernetes, protobuf, pydantic, PyYAML, requests-toolbelt, strip-hints, tabulate, typer, uritemplate, urllib3\n",
      "Required-by: google-cloud-pipeline-components\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show kfp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfc78f1f-ef16-4581-8b80-edebe843c37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: google-cloud-pipeline-components\n",
      "Version: 1.0.23\n",
      "Summary: This SDK enables a set of First Party (Google owned) pipeline components that allow users to take their experience from Vertex AI SDK and other Google Cloud services and create a corresponding pipeline using KFP or Managed Pipelines.\n",
      "Home-page: https://github.com/kubeflow/pipelines/tree/master/components/google-cloud\n",
      "Author: The Google Cloud Pipeline Components authors\n",
      "Author-email: google-cloud-pipeline-components@google.com\n",
      "License: Apache License 2.0\n",
      "Location: /opt/conda/lib/python3.10/site-packages\n",
      "Requires: google-api-core, google-cloud-aiplatform, google-cloud-notebooks, google-cloud-storage, kfp\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show google-cloud-pipeline-components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f321852-cd6b-4168-b77b-7a91c913ffe6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kfp==1.8.22\n",
      "  Using cached kfp-1.8.22-py3-none-any.whl\n",
      "Requirement already satisfied: absl-py<2,>=0.9 in /opt/conda/lib/python3.10/site-packages (from kfp==1.8.22) (1.4.0)\n",
      "Requirement already satisfied: PyYAML<7,>=5.3 in /opt/conda/lib/python3.10/site-packages (from kfp==1.8.22) (6.0.2)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.10/site-packages (from kfp==1.8.22) (1.34.1)\n",
      "Requirement already satisfied: google-cloud-storage<3,>=1.20.0 in /opt/conda/lib/python3.10/site-packages (from kfp==1.8.22) (2.14.0)\n",
      "Requirement already satisfied: kubernetes<26,>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp==1.8.22) (25.3.0)\n",
      "Requirement already satisfied: google-api-python-client<2,>=1.7.8 in /opt/conda/lib/python3.10/site-packages (from kfp==1.8.22) (1.8.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.1 in /opt/conda/lib/python3.10/site-packages (from kfp==1.8.22) (2.37.0)\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from kfp==1.8.22) (0.10.1)\n",
      "Requirement already satisfied: cloudpickle<3,>=2.0.0 in ./.local/lib/python3.10/site-packages (from kfp==1.8.22) (2.2.1)\n",
      "Requirement already satisfied: kfp-server-api<2.0.0,>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from kfp==1.8.22) (1.8.5)\n",
      "Requirement already satisfied: jsonschema<5,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from kfp==1.8.22) (4.23.0)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /opt/conda/lib/python3.10/site-packages (from kfp==1.8.22) (0.9.0)\n",
      "Requirement already satisfied: click<9,>=7.1.2 in /opt/conda/lib/python3.10/site-packages (from kfp==1.8.22) (8.1.8)\n",
      "Requirement already satisfied: Deprecated<2,>=1.2.7 in /opt/conda/lib/python3.10/site-packages (from kfp==1.8.22) (1.2.15)\n",
      "Requirement already satisfied: strip-hints<1,>=0.1.8 in /opt/conda/lib/python3.10/site-packages (from kfp==1.8.22) (0.1.12)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /opt/conda/lib/python3.10/site-packages (from kfp==1.8.22) (0.16)\n",
      "Requirement already satisfied: kfp-pipeline-spec<0.2.0,>=0.1.16 in /opt/conda/lib/python3.10/site-packages (from kfp==1.8.22) (0.1.16)\n",
      "Requirement already satisfied: fire<1,>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from kfp==1.8.22) (0.7.0)\n",
      "Requirement already satisfied: protobuf<4,>=3.13.0 in /opt/conda/lib/python3.10/site-packages (from kfp==1.8.22) (3.20.3)\n",
      "Requirement already satisfied: uritemplate<4,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from kfp==1.8.22) (3.0.1)\n",
      "Requirement already satisfied: urllib3<2 in /opt/conda/lib/python3.10/site-packages (from kfp==1.8.22) (1.26.20)\n",
      "Requirement already satisfied: pydantic<2,>=1.8.2 in /opt/conda/lib/python3.10/site-packages (from kfp==1.8.22) (1.10.21)\n",
      "Requirement already satisfied: typer<1.0,>=0.3.2 in /opt/conda/lib/python3.10/site-packages (from kfp==1.8.22) (0.15.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.10/site-packages (from Deprecated<2,>=1.2.7->kfp==1.8.22) (1.17.0)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from fire<1,>=0.3.1->kfp==1.8.22) (2.5.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==1.8.22) (1.66.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==1.8.22) (2.32.3)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.8.22) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.8.22) (0.2.0)\n",
      "Requirement already satisfied: six<2dev,>=1.6.1 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.8.22) (1.17.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.1->kfp==1.8.22) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.1->kfp==1.8.22) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.1->kfp==1.8.22) (4.9)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3,>=1.20.0->kfp==1.8.22) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media>=2.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3,>=1.20.0->kfp==1.8.22) (2.7.2)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3,>=1.20.0->kfp==1.8.22) (1.6.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema<5,>=3.0.1->kfp==1.8.22) (24.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema<5,>=3.0.1->kfp==1.8.22) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema<5,>=3.0.1->kfp==1.8.22) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema<5,>=3.0.1->kfp==1.8.22) (0.22.3)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.22) (2024.12.14)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.22) (2.9.0.post0)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes<26,>=8.0.0->kfp==1.8.22) (75.6.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes<26,>=8.0.0->kfp==1.8.22) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.10/site-packages (from kubernetes<26,>=8.0.0->kfp==1.8.22) (2.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<2,>=1.8.2->kfp==1.8.22) (4.12.2)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from strip-hints<1,>=0.1.8->kfp==1.8.22) (0.45.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.3.2->kfp==1.8.22) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.3.2->kfp==1.8.22) (13.9.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/lib/python3.10/site-packages (from httplib2<1dev,>=0.9.2->google-api-python-client<2,>=1.7.8->kfp==1.8.22) (3.2.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.1->kfp==1.8.22) (0.6.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==1.8.22) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==1.8.22) (3.10)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.3.2->kfp==1.8.22) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.3.2->kfp==1.8.22) (2.18.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib->kubernetes<26,>=8.0.0->kfp==1.8.22) (3.2.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.3.2->kfp==1.8.22) (0.1.2)\n",
      "Installing collected packages: kfp\n",
      "  Attempting uninstall: kfp\n",
      "    Found existing installation: kfp 2.5.0\n",
      "    Uninstalling kfp-2.5.0:\n",
      "      Successfully uninstalled kfp-2.5.0\n",
      "Successfully installed kfp-1.8.22\n",
      "Collecting google-cloud-pipeline-components==1.0.23\n",
      "  Downloading google_cloud_pipeline_components-1.0.23-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.10/site-packages (from google-cloud-pipeline-components==1.0.23) (1.34.1)\n",
      "Collecting google-cloud-storage<2,>=1.20.0 (from google-cloud-pipeline-components==1.0.23)\n",
      "  Downloading google_cloud_storage-1.44.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: kfp<2.0.0,>=1.8.9 in /opt/conda/lib/python3.10/site-packages (from google-cloud-pipeline-components==1.0.23) (1.8.22)\n",
      "Requirement already satisfied: google-cloud-aiplatform<2,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-pipeline-components==1.0.23) (1.75.0)\n",
      "Collecting google-cloud-notebooks>=0.4.0 (from google-cloud-pipeline-components==1.0.23)\n",
      "  Downloading google_cloud_notebooks-1.13.0-py2.py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components==1.0.23) (1.66.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components==1.0.23) (3.20.3)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components==1.0.23) (2.37.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components==1.0.23) (2.32.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2,>=1.11.0->google-cloud-pipeline-components==1.0.23) (1.25.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2,>=1.11.0->google-cloud-pipeline-components==1.0.23) (24.2)\n",
      "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2,>=1.11.0->google-cloud-pipeline-components==1.0.23) (3.25.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2,>=1.11.0->google-cloud-pipeline-components==1.0.23) (1.14.0)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2,>=1.11.0->google-cloud-pipeline-components==1.0.23) (2.0.6)\n",
      "Requirement already satisfied: pydantic<3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2,>=1.11.0->google-cloud-pipeline-components==1.0.23) (1.10.21)\n",
      "Requirement already satisfied: docstring-parser<1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2,>=1.11.0->google-cloud-pipeline-components==1.0.23) (0.16)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.10/site-packages (from google-cloud-notebooks>=0.4.0->google-cloud-pipeline-components==1.0.23) (0.13.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<2,>=1.20.0->google-cloud-pipeline-components==1.0.23) (1.17.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<2,>=1.20.0->google-cloud-pipeline-components==1.0.23) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<2,>=1.20.0->google-cloud-pipeline-components==1.0.23) (2.7.2)\n",
      "Requirement already satisfied: absl-py<2,>=0.9 in /opt/conda/lib/python3.10/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components==1.0.23) (1.4.0)\n",
      "Requirement already satisfied: PyYAML<7,>=5.3 in /opt/conda/lib/python3.10/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components==1.0.23) (6.0.2)\n",
      "Requirement already satisfied: kubernetes<26,>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components==1.0.23) (25.3.0)\n",
      "Requirement already satisfied: google-api-python-client<2,>=1.7.8 in /opt/conda/lib/python3.10/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components==1.0.23) (1.8.0)\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components==1.0.23) (0.10.1)\n",
      "Requirement already satisfied: cloudpickle<3,>=2.0.0 in ./.local/lib/python3.10/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components==1.0.23) (2.2.1)\n",
      "Requirement already satisfied: kfp-server-api<2.0.0,>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components==1.0.23) (1.8.5)\n",
      "Requirement already satisfied: jsonschema<5,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components==1.0.23) (4.23.0)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /opt/conda/lib/python3.10/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components==1.0.23) (0.9.0)\n",
      "Requirement already satisfied: click<9,>=7.1.2 in /opt/conda/lib/python3.10/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components==1.0.23) (8.1.8)\n",
      "Requirement already satisfied: Deprecated<2,>=1.2.7 in /opt/conda/lib/python3.10/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components==1.0.23) (1.2.15)\n",
      "Requirement already satisfied: strip-hints<1,>=0.1.8 in /opt/conda/lib/python3.10/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components==1.0.23) (0.1.12)\n",
      "Requirement already satisfied: kfp-pipeline-spec<0.2.0,>=0.1.16 in /opt/conda/lib/python3.10/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components==1.0.23) (0.1.16)\n",
      "Requirement already satisfied: fire<1,>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components==1.0.23) (0.7.0)\n",
      "Requirement already satisfied: uritemplate<4,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components==1.0.23) (3.0.1)\n",
      "Requirement already satisfied: urllib3<2 in /opt/conda/lib/python3.10/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components==1.0.23) (1.26.20)\n",
      "Requirement already satisfied: typer<1.0,>=0.3.2 in /opt/conda/lib/python3.10/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components==1.0.23) (0.15.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.10/site-packages (from Deprecated<2,>=1.2.7->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components==1.0.23) (1.17.0)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from fire<1,>=0.3.1->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components==1.0.23) (2.5.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in ./.local/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2,>=1.11.0->google-cloud-pipeline-components==1.0.23) (1.65.5)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2,>=1.11.0->google-cloud-pipeline-components==1.0.23) (1.48.2)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client<2,>=1.7.8->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components==1.0.23) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client<2,>=1.7.8->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components==1.0.23) (0.2.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components==1.0.23) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components==1.0.23) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components==1.0.23) (4.9)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2,>=1.11.0->google-cloud-pipeline-components==1.0.23) (2.9.0.post0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<2,>=1.20.0->google-cloud-pipeline-components==1.0.23) (1.6.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema<5,>=3.0.1->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components==1.0.23) (24.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema<5,>=3.0.1->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components==1.0.23) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema<5,>=3.0.1->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components==1.0.23) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema<5,>=3.0.1->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components==1.0.23) (0.22.3)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components==1.0.23) (2024.12.14)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes<26,>=8.0.0->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components==1.0.23) (75.6.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes<26,>=8.0.0->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components==1.0.23) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.10/site-packages (from kubernetes<26,>=8.0.0->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components==1.0.23) (2.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform<2,>=1.11.0->google-cloud-pipeline-components==1.0.23) (4.12.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components==1.0.23) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components==1.0.23) (3.10)\n",
      "Requirement already satisfied: numpy<3,>=1.14 in /opt/conda/lib/python3.10/site-packages (from shapely<3.0.0dev->google-cloud-aiplatform<2,>=1.11.0->google-cloud-pipeline-components==1.0.23) (1.26.4)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from strip-hints<1,>=0.1.8->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components==1.0.23) (0.45.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.3.2->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components==1.0.23) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.3.2->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components==1.0.23) (13.9.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/lib/python3.10/site-packages (from httplib2<1dev,>=0.9.2->google-api-python-client<2,>=1.7.8->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components==1.0.23) (3.2.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components==1.0.23) (0.6.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.3.2->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components==1.0.23) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.3.2->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components==1.0.23) (2.18.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib->kubernetes<26,>=8.0.0->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components==1.0.23) (3.2.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.3.2->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components==1.0.23) (0.1.2)\n",
      "Downloading google_cloud_pipeline_components-1.0.23-py3-none-any.whl (742 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m742.7/742.7 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_cloud_notebooks-1.13.0-py2.py3-none-any.whl (299 kB)\n",
      "Downloading google_cloud_storage-1.44.0-py2.py3-none-any.whl (106 kB)\n",
      "Installing collected packages: google-cloud-storage, google-cloud-notebooks, google-cloud-pipeline-components\n",
      "  Attempting uninstall: google-cloud-storage\n",
      "    Found existing installation: google-cloud-storage 2.14.0\n",
      "    Uninstalling google-cloud-storage-2.14.0:\n",
      "      Successfully uninstalled google-cloud-storage-2.14.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 1.9.0 requires google-cloud-storage>=2.0.0, but you have google-cloud-storage 1.44.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed google-cloud-notebooks-1.13.0 google-cloud-pipeline-components-1.0.23 google-cloud-storage-1.44.0\n"
     ]
    }
   ],
   "source": [
    "# Resolvou o problema!\n",
    "\n",
    "!pip install kfp==1.8.22\n",
    "!pip install google-cloud-pipeline-components==1.0.23\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "beaf787a-b832-40e3-9c56-572209aa617e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kfp in /opt/conda/lib/python3.10/site-packages (2.10.1)\n",
      "Collecting kfp\n",
      "  Using cached kfp-2.11.0-py3-none-any.whl\n",
      "Requirement already satisfied: click<9,>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp) (8.1.8)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /opt/conda/lib/python3.10/site-packages (from kfp) (0.16)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in ./.local/lib/python3.10/site-packages (from kfp) (2.24.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.1 in /opt/conda/lib/python3.10/site-packages (from kfp) (2.37.0)\n",
      "Requirement already satisfied: google-cloud-storage<3,>=2.2.1 in ./.local/lib/python3.10/site-packages (from kfp) (2.19.0)\n",
      "Collecting kfp-pipeline-spec==0.6.0 (from kfp)\n",
      "  Using cached kfp_pipeline_spec-0.6.0-py3-none-any.whl.metadata (293 bytes)\n",
      "Requirement already satisfied: kfp-server-api<2.4.0,>=2.1.0 in ./.local/lib/python3.10/site-packages (from kfp) (2.3.0)\n",
      "Requirement already satisfied: kubernetes<31,>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp) (26.1.0)\n",
      "Requirement already satisfied: protobuf<5,>=4.21.1 in ./.local/lib/python3.10/site-packages (from kfp) (4.25.6)\n",
      "Requirement already satisfied: PyYAML<7,>=5.3 in /opt/conda/lib/python3.10/site-packages (from kfp) (6.0.2)\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from kfp) (0.10.1)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /opt/conda/lib/python3.10/site-packages (from kfp) (0.9.0)\n",
      "Requirement already satisfied: urllib3<2.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp) (1.26.20)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (1.66.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (1.25.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.1->kfp) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.1->kfp) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.1->kfp) (4.9)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3,>=2.2.1->kfp) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media>=2.7.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3,>=2.2.1->kfp) (2.7.2)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3,>=2.2.1->kfp) (1.6.0)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.10/site-packages (from kfp-server-api<2.4.0,>=2.1.0->kfp) (1.17.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from kfp-server-api<2.4.0,>=2.1.0->kfp) (2024.12.14)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from kfp-server-api<2.4.0,>=2.1.0->kfp) (2.9.0.post0)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes<31,>=8.0.0->kfp) (75.6.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes<31,>=8.0.0->kfp) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.10/site-packages (from kubernetes<31,>=8.0.0->kfp) (2.0.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.1->kfp) (0.6.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (3.10)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib->kubernetes<31,>=8.0.0->kfp) (3.2.2)\n",
      "Using cached kfp_pipeline_spec-0.6.0-py3-none-any.whl (9.1 kB)\n",
      "Installing collected packages: kfp-pipeline-spec, kfp\n",
      "\u001b[33m  WARNING: The scripts dsl-compile and kfp are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-cloud-pipeline-components 2.19.0 requires kfp<2.11.0,>=2.6.0, but you have kfp 2.11.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed kfp-2.11.0 kfp-pipeline-spec-0.6.0\n",
      "Requirement already satisfied: google-cloud-pipeline-components in /opt/conda/lib/python3.10/site-packages (2.19.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in ./.local/lib/python3.10/site-packages (from google-cloud-pipeline-components) (2.24.1)\n",
      "Collecting kfp<2.11.0,>=2.6.0 (from google-cloud-pipeline-components)\n",
      "  Using cached kfp-2.10.1-py3-none-any.whl\n",
      "Requirement already satisfied: google-cloud-aiplatform<2,>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-pipeline-components) (1.75.0)\n",
      "Requirement already satisfied: Jinja2<4,>=3.1.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-pipeline-components) (3.1.5)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (1.66.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in ./.local/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (4.25.6)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (1.25.0)\n",
      "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (2.37.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (2.32.3)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (24.2)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in ./.local/lib/python3.10/site-packages (from google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (2.19.0)\n",
      "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (3.25.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (1.14.0)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (2.0.6)\n",
      "Requirement already satisfied: pydantic<3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (2.10.4)\n",
      "Requirement already satisfied: docstring-parser<1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (0.16)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2<4,>=3.1.2->google-cloud-pipeline-components) (3.0.2)\n",
      "Requirement already satisfied: click<9,>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp<2.11.0,>=2.6.0->google-cloud-pipeline-components) (8.1.8)\n",
      "Collecting kfp-pipeline-spec==0.5.0 (from kfp<2.11.0,>=2.6.0->google-cloud-pipeline-components)\n",
      "  Using cached kfp_pipeline_spec-0.5.0-py3-none-any.whl.metadata (293 bytes)\n",
      "Requirement already satisfied: kfp-server-api<2.4.0,>=2.1.0 in ./.local/lib/python3.10/site-packages (from kfp<2.11.0,>=2.6.0->google-cloud-pipeline-components) (2.3.0)\n",
      "Requirement already satisfied: kubernetes<31,>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp<2.11.0,>=2.6.0->google-cloud-pipeline-components) (26.1.0)\n",
      "Requirement already satisfied: PyYAML<7,>=5.3 in /opt/conda/lib/python3.10/site-packages (from kfp<2.11.0,>=2.6.0->google-cloud-pipeline-components) (6.0.2)\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from kfp<2.11.0,>=2.6.0->google-cloud-pipeline-components) (0.10.1)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /opt/conda/lib/python3.10/site-packages (from kfp<2.11.0,>=2.6.0->google-cloud-pipeline-components) (0.9.0)\n",
      "Requirement already satisfied: urllib3<2.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp<2.11.0,>=2.6.0->google-cloud-pipeline-components) (1.26.20)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in ./.local/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (1.65.5)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (1.48.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (4.9)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (2.7.2)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (2.9.0.post0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (0.13.1)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (1.6.0)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.10/site-packages (from kfp-server-api<2.4.0,>=2.1.0->kfp<2.11.0,>=2.6.0->google-cloud-pipeline-components) (1.17.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from kfp-server-api<2.4.0,>=2.1.0->kfp<2.11.0,>=2.6.0->google-cloud-pipeline-components) (2024.12.14)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes<31,>=8.0.0->kfp<2.11.0,>=2.6.0->google-cloud-pipeline-components) (75.6.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes<31,>=8.0.0->kfp<2.11.0,>=2.6.0->google-cloud-pipeline-components) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.10/site-packages (from kubernetes<31,>=8.0.0->kfp<2.11.0,>=2.6.0->google-cloud-pipeline-components) (2.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (4.12.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (3.10)\n",
      "Requirement already satisfied: numpy<3,>=1.14 in /opt/conda/lib/python3.10/site-packages (from shapely<3.0.0dev->google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (1.26.4)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib->kubernetes<31,>=8.0.0->kfp<2.11.0,>=2.6.0->google-cloud-pipeline-components) (3.2.2)\n",
      "Using cached kfp_pipeline_spec-0.5.0-py3-none-any.whl (9.1 kB)\n",
      "Installing collected packages: kfp-pipeline-spec, kfp\n",
      "  Attempting uninstall: kfp-pipeline-spec\n",
      "    Found existing installation: kfp-pipeline-spec 0.6.0\n",
      "    Uninstalling kfp-pipeline-spec-0.6.0:\n",
      "      Successfully uninstalled kfp-pipeline-spec-0.6.0\n",
      "  Attempting uninstall: kfp\n",
      "    Found existing installation: kfp 2.11.0\n",
      "    Uninstalling kfp-2.11.0:\n",
      "      Successfully uninstalled kfp-2.11.0\n",
      "\u001b[33m  WARNING: The scripts dsl-compile and kfp are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed kfp-2.10.1 kfp-pipeline-spec-0.5.0\n",
      "Requirement already satisfied: apache-beam[gcp] in ./.local/lib/python3.10/site-packages (2.62.0)\n",
      "Requirement already satisfied: crcmod<2.0,>=1.7 in ./.local/lib/python3.10/site-packages (from apache-beam[gcp]) (1.7)\n",
      "Requirement already satisfied: orjson<4,>=3.9.7 in ./.local/lib/python3.10/site-packages (from apache-beam[gcp]) (3.10.15)\n",
      "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in ./.local/lib/python3.10/site-packages (from apache-beam[gcp]) (0.3.1.1)\n",
      "Requirement already satisfied: cloudpickle~=2.2.1 in ./.local/lib/python3.10/site-packages (from apache-beam[gcp]) (2.2.1)\n",
      "Requirement already satisfied: fastavro<2,>=0.23.6 in ./.local/lib/python3.10/site-packages (from apache-beam[gcp]) (1.10.0)\n",
      "Requirement already satisfied: fasteners<1.0,>=0.3 in ./.local/lib/python3.10/site-packages (from apache-beam[gcp]) (0.19)\n",
      "Requirement already satisfied: grpcio!=1.48.0,!=1.59.*,!=1.60.*,!=1.61.*,!=1.62.0,!=1.62.1,<1.66.0,<2,>=1.33.1 in ./.local/lib/python3.10/site-packages (from apache-beam[gcp]) (1.65.5)\n",
      "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in ./.local/lib/python3.10/site-packages (from apache-beam[gcp]) (2.7.3)\n",
      "Requirement already satisfied: httplib2<0.23.0,>=0.8 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (0.22.0)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (4.23.0)\n",
      "Requirement already satisfied: jsonpickle<4.0.0,>=3.0.0 in ./.local/lib/python3.10/site-packages (from apache-beam[gcp]) (3.4.2)\n",
      "Requirement already satisfied: numpy<2.3.0,>=1.14.3 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (1.26.4)\n",
      "Requirement already satisfied: objsize<0.8.0,>=0.6.1 in ./.local/lib/python3.10/site-packages (from apache-beam[gcp]) (0.7.1)\n",
      "Requirement already satisfied: packaging>=22.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (24.2)\n",
      "Requirement already satisfied: pymongo<5.0.0,>=3.8.0 in ./.local/lib/python3.10/site-packages (from apache-beam[gcp]) (4.11.1)\n",
      "Requirement already satisfied: proto-plus<2,>=1.7.1 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (1.25.0)\n",
      "Requirement already satisfied: protobuf!=4.0.*,!=4.21.*,!=4.22.0,!=4.23.*,!=4.24.*,<6.0.0.dev0,>=3.20.3 in ./.local/lib/python3.10/site-packages (from apache-beam[gcp]) (4.25.6)\n",
      "Requirement already satisfied: pydot<2,>=1.2.0 in ./.local/lib/python3.10/site-packages (from apache-beam[gcp]) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2018.3 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (2024.2)\n",
      "Requirement already satisfied: redis<6,>=5.0.0 in ./.local/lib/python3.10/site-packages (from apache-beam[gcp]) (5.2.1)\n",
      "Requirement already satisfied: regex>=2020.6.8 in ./.local/lib/python3.10/site-packages (from apache-beam[gcp]) (2024.11.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (2.32.3)\n",
      "Requirement already satisfied: sortedcontainers>=2.4.0 in ./.local/lib/python3.10/site-packages (from apache-beam[gcp]) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (4.12.2)\n",
      "Requirement already satisfied: zstandard<1,>=0.18.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (0.23.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=3.12 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (6.0.2)\n",
      "Requirement already satisfied: pyarrow<17.0.0,>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix<1 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (0.6)\n",
      "Requirement already satisfied: cachetools<6,>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (5.5.0)\n",
      "Requirement already satisfied: google-api-core<3,>=2.0.0 in ./.local/lib/python3.10/site-packages (from apache-beam[gcp]) (2.24.1)\n",
      "Requirement already satisfied: google-apitools<0.5.32,>=0.5.31 in ./.local/lib/python3.10/site-packages (from apache-beam[gcp]) (0.5.31)\n",
      "Requirement already satisfied: google-auth<3,>=1.18.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (2.37.0)\n",
      "Requirement already satisfied: google-auth-httplib2<0.3.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (0.2.0)\n",
      "Requirement already satisfied: google-cloud-datastore<3,>=2.0.0 in ./.local/lib/python3.10/site-packages (from apache-beam[gcp]) (2.20.2)\n",
      "Requirement already satisfied: google-cloud-pubsub<3,>=2.1.0 in ./.local/lib/python3.10/site-packages (from apache-beam[gcp]) (2.28.0)\n",
      "Requirement already satisfied: google-cloud-pubsublite<2,>=1.2.0 in ./.local/lib/python3.10/site-packages (from apache-beam[gcp]) (1.11.1)\n",
      "Requirement already satisfied: google-cloud-storage<3,>=2.18.2 in ./.local/lib/python3.10/site-packages (from apache-beam[gcp]) (2.19.0)\n",
      "Requirement already satisfied: google-cloud-bigquery<4,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (3.25.0)\n",
      "Requirement already satisfied: google-cloud-bigquery-storage<3,>=2.6.3 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (2.27.0)\n",
      "Requirement already satisfied: google-cloud-core<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (2.4.1)\n",
      "Requirement already satisfied: google-cloud-bigtable<3,>=2.19.0 in ./.local/lib/python3.10/site-packages (from apache-beam[gcp]) (2.28.1)\n",
      "Requirement already satisfied: google-cloud-spanner<4,>=3.0.0 in ./.local/lib/python3.10/site-packages (from apache-beam[gcp]) (3.51.0)\n",
      "Requirement already satisfied: google-cloud-dlp<4,>=3.0.0 in ./.local/lib/python3.10/site-packages (from apache-beam[gcp]) (3.27.0)\n",
      "Requirement already satisfied: google-cloud-language<3,>=2.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (2.16.0)\n",
      "Requirement already satisfied: google-cloud-videointelligence<3,>=2.0 in ./.local/lib/python3.10/site-packages (from apache-beam[gcp]) (2.16.0)\n",
      "Requirement already satisfied: google-cloud-vision<4,>=2 in ./.local/lib/python3.10/site-packages (from apache-beam[gcp]) (3.10.0)\n",
      "Requirement already satisfied: google-cloud-recommendations-ai<0.11.0,>=0.1.0 in ./.local/lib/python3.10/site-packages (from apache-beam[gcp]) (0.10.15)\n",
      "Requirement already satisfied: google-cloud-aiplatform<2.0,>=1.26.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (1.75.0)\n",
      "Requirement already satisfied: keyrings.google-artifactregistry-auth in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]) (1.1.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core<3,>=2.0.0->apache-beam[gcp]) (1.66.0)\n",
      "Requirement already satisfied: oauth2client>=1.4.12 in /opt/conda/lib/python3.10/site-packages (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]) (4.1.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]) (1.17.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]) (4.9)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]) (1.14.0)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]) (2.0.6)\n",
      "Requirement already satisfied: pydantic<3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]) (2.10.4)\n",
      "Requirement already satisfied: docstring-parser<1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]) (0.16)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery<4,>=2.0.0->apache-beam[gcp]) (2.7.2)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigtable<3,>=2.19.0->apache-beam[gcp]) (0.13.1)\n",
      "Requirement already satisfied: grpcio-status>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]) (1.48.2)\n",
      "Requirement already satisfied: opentelemetry-api>=1.27.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.27.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]) (1.27.0)\n",
      "Requirement already satisfied: overrides<8.0.0,>=6.0.1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]) (7.7.0)\n",
      "Requirement already satisfied: sqlparse>=0.4.4 in /opt/conda/lib/python3.10/site-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]) (0.5.3)\n",
      "Requirement already satisfied: grpc-interceptor>=0.15.4 in ./.local/lib/python3.10/site-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]) (0.15.4)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3,>=2.18.2->apache-beam[gcp]) (1.6.0)\n",
      "Requirement already satisfied: docopt in ./.local/lib/python3.10/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]) (0.6.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/lib/python3.10/site-packages (from httplib2<0.23.0,>=0.8->apache-beam[gcp]) (3.2.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam[gcp]) (24.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam[gcp]) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam[gcp]) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam[gcp]) (0.22.3)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in ./.local/lib/python3.10/site-packages (from pymongo<5.0.0,>=3.8.0->apache-beam[gcp]) (2.7.0)\n",
      "Requirement already satisfied: async-timeout>=4.0.3 in /opt/conda/lib/python3.10/site-packages (from redis<6,>=5.0.0->apache-beam[gcp]) (5.0.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]) (2024.12.14)\n",
      "Requirement already satisfied: keyring in /opt/conda/lib/python3.10/site-packages (from keyrings.google-artifactregistry-auth->apache-beam[gcp]) (25.5.0)\n",
      "Requirement already satisfied: pluggy in /opt/conda/lib/python3.10/site-packages (from keyrings.google-artifactregistry-auth->apache-beam[gcp]) (1.5.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.10/site-packages (from oauth2client>=1.4.12->google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]) (0.6.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api>=1.27.0->google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]) (1.2.15)\n",
      "Requirement already satisfied: importlib-metadata<=8.4.0,>=6.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api>=1.27.0->google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]) (8.4.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.48b0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-sdk>=1.27.0->google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]) (0.48b0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]) (2.27.2)\n",
      "Requirement already satisfied: jaraco.classes in /opt/conda/lib/python3.10/site-packages (from keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]) (3.4.0)\n",
      "Requirement already satisfied: jaraco.functools in /opt/conda/lib/python3.10/site-packages (from keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]) (4.1.0)\n",
      "Requirement already satisfied: jaraco.context in /opt/conda/lib/python3.10/site-packages (from keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]) (6.0.1)\n",
      "Requirement already satisfied: SecretStorage>=3.2 in /opt/conda/lib/python3.10/site-packages (from keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]) (3.3.3)\n",
      "Requirement already satisfied: jeepney>=0.4.2 in /opt/conda/lib/python3.10/site-packages (from keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]) (0.8.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.10/site-packages (from deprecated>=1.2.6->opentelemetry-api>=1.27.0->google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]) (1.17.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api>=1.27.0->google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]) (3.21.0)\n",
      "Requirement already satisfied: cryptography>=2.0 in /opt/conda/lib/python3.10/site-packages (from SecretStorage>=3.2->keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]) (44.0.0)\n",
      "Requirement already satisfied: more-itertools in /opt/conda/lib/python3.10/site-packages (from jaraco.classes->keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]) (10.5.0)\n",
      "Requirement already satisfied: backports.tarfile in /opt/conda/lib/python3.10/site-packages (from jaraco.context->keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]) (1.2.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=2.0->SecretStorage>=3.2->keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=2.0->SecretStorage>=3.2->keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]) (2.22)\n"
     ]
    }
   ],
   "source": [
    "USER_FLAG = \"--user\"\n",
    "!pip3 install {USER_FLAG} kfp --upgrade\n",
    "#!pip3 install {USER FLAG} google-cloud-pipeline-components --upgrade\n",
    "#!pip3 install {USER_FLAG} -U google-cloud-pipeline-components\n",
    "!pip3 install {USER_FLAG} 'apache-beam[gcp]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf36cf0-cd7e-4ad0-8cf4-de8106a60709",
   "metadata": {},
   "source": [
    "USER_FLAG = \"--user\" # Primeira linha força a instalação\n",
    "!pip3 install {USER_FLAG} kfp --upgrade # aqui estamos instalado o kfp\n",
    "!pip3 install {USER FLAG} google cloud pipeline components --upgrade # componentes do próprio gcp\n",
    "!pip3 install {USER_FLAG} 'apache_beas[gcp] # Essa aqui para evitar alguns problemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9c02389-d8fc-4a0d-82c6-1aae3d6d2257",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install google_cloud_pipeline_components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00c6d6a-3594-42ee-9823-f908b8b451cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! python3 -c 'import' kfp: print('KFP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10e9bc9-46bd-4739-9c90-146a8029fcf4",
   "metadata": {},
   "source": [
    "# Import bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f58b46ff-86b7-429e-9750-b831693b1f95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "\n",
    "from typing import NamedTuple # Biblioteca de formatação\n",
    "\n",
    "from kfp.v2 import dsl # v2significa que está sendo desenvolvida\n",
    "from kfp.v2.dsl import ( pipeline, \n",
    "                       Artifact, \n",
    "                       Dataset, \n",
    "                       Input, \n",
    "                       Model, \n",
    "                       Output, \n",
    "                       Metrics,\n",
    "                       ClassificationMetrics, \n",
    "                       component, \n",
    "                       Markdown)\n",
    "\n",
    "from kfp.v2 import compiler # compilador\n",
    "\n",
    "from google.cloud import aiplatform # inicializar o projeto em cima do aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "\n",
    "import google.cloud.aiplatform as gcc_aip\n",
    "\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip # Problema Resolvido cocm a nova instalação\n",
    "\n",
    "PIPELINE_ROOT = \"gs://pipeline_ecommerce\" # Salvará os arquivos aqui!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf932e1-fa65-4937-8876-24a289dfa5c3",
   "metadata": {},
   "source": [
    "# 1. Componente: Captura dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0af9425f-c616-4747-bebf-a53f43df374b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos criar e instalar alguns components\n",
    "@component( packages_to_install = [\"pandas\", \"google.cloud.bigquery\", \"db.dtypes\", \"pandas.gbq\"], \n",
    "          base_image = \"python:3.10.6\",\n",
    "          output_component_file = \"captura_dados_ecommerce.yaml\")\n",
    "\n",
    "def captura_dados(dataset: Output[Dataset]):\n",
    "    import os \n",
    "    import pandas as pd\n",
    "    \n",
    "    \n",
    "    project_number = os.enviros['CLOUD_NO_PROJECT_ID']\n",
    "    \n",
    "    query_sql = \"\"\"\n",
    "                SELECT * FROM 'braided-period-442813-v0.ecommerce_cluster_insider.dados_ecommerce_iterm'\n",
    "                LIMIT 1000\n",
    "                \"\"\"\n",
    "    df_raw  = pd.read_gbq(query       = query_sql, \n",
    "                          project_id  = project_number,\n",
    "                          use_bqstorage_api = True)\n",
    "    df_raw.to_csv(dataset.path + '.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da972d76-a02d-4836-9772-9c9b57ed3053",
   "metadata": {},
   "source": [
    "# 2. Componente: Preparação dos dados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "595db006-f589-4403-abb7-4f44fdda6f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component( packages_to_install = ['pandas'], \n",
    "                     base_image = \"python:3.10.6\",\n",
    "           output_component_file= \"data_preparation_ecommerce.yaml\")\n",
    "def data_preparation( dataset: Input[Dataset],\n",
    "                     df_filtered: Output[Dataset],\n",
    "                     df_purchases: Output[Dataset], \n",
    "                     df_returns: Output[Dataset]):\n",
    "    from typing import Taple\n",
    "    import pandas as pd\n",
    "    \n",
    "    def keep_features(dataframe: pd.DataFrame, keep_columns: list) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Retorna um DataFrame com as colunas especificadas em keep_colunas\n",
    "\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): O DataFrame a ser processado.\n",
    "            keep_columns (list): A lista de nomes de colunas a serem mantidas no DataFrame resultante. \n",
    "        Returns:\n",
    "            pd.DataFrame: O DataFrame resultante com apenas as colunas especificadas em keep_columns.\n",
    "        \"\"\"\n",
    "    return dataframe[keep_columns]\n",
    "    \n",
    "    def column_to_init( dataframe:pd.DataFrame, column_name:str) -> bool:\n",
    "        \"\"\"\n",
    "            Descrição:\n",
    "                Converte a coluna especificada em um datafreme parao tipo int\n",
    "\n",
    "            Args: \n",
    "                 dataframe(pd.DataFrame): O dataframe a ser processado.\n",
    "                 column_name (str): O nome da coluna a ser convertida.\n",
    "            Returns:\n",
    "                 bool: True se a conversão foi bem sucedida. False caso contrário.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            dataframe[column_name] = dataframe[column_name].astype(int)\n",
    "        except (ValueError, typerError):\n",
    "            return False\n",
    "            # Lídar com valores ausentes e conversões inválidas\n",
    "            \n",
    "        #if column_name not in dataframe.columns:\n",
    "         #   raise ValueError(f'Coluna {column_name} não encontrado')\n",
    "        #try:    \n",
    "        #    dataframe[column_name] = dataframe[column_name].astype(int)\n",
    "        #except:\n",
    "        #    raise ValueError(f'Não foi possível converter a coluna {column_name} para inteiro.')\n",
    "        return True\n",
    "    \n",
    "    def column_to_date( dataframe: pd.DataFrame, column_name: str, date_format: str = None) -> bool:\n",
    "        \"\"\"\n",
    "        Descrição:\n",
    "            Converte a coluna especificada em um datafreme parao tipo data\n",
    "\n",
    "        Args: \n",
    "             dataframe(pd.DataFrame): O dataframe a ser processado.\n",
    "             column_name (str): O nome da coluna a ser convertida.\n",
    "        Returns:\n",
    "             bool: True se a conversão foi bem sucedida. False caso contrário.\n",
    "        \"\"\"\n",
    "        if column_name not in dataframe.columns:\n",
    "            raise ValueError(f'Coluna {column_name} não encontrado no dataframe de entrada')\n",
    "\n",
    "        try:  \n",
    "            if date_format:\n",
    "                dataframe[column_name] = pd.to_datetime(dataframe[column_name], format = date_format)\n",
    "            else:\n",
    "                dataframe[column_name] = pd.to_datetime(dataframe[column_name])\n",
    "        except (ValueError, TypeError):\n",
    "            raise ValueError(f'Could not convert column {column_name}')\n",
    "        # Retorna True se a conversão for bem sucedida\n",
    "        return True\n",
    "\n",
    "\n",
    "    def change_column_type(dataframe_raw: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Changes the data type of column g in a DataFrame\n",
    "\n",
    "        Args:\n",
    "            dataframe_raw: A pandas DataFrame\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        column_to_init( dataframe_raw, 'CustomerID')\n",
    "        column_to_date( dataframe_raw, 'InvoiceDate')\n",
    "\n",
    "    def filterig_features(dataframe_raw: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Descrição\n",
    "            Filters and preprocesses the input Dataframe.\n",
    "\n",
    "        Args:\n",
    "            dataframe_raw: A pandas Dataframe containing_raw sales data.\n",
    "        Retornos: \n",
    "            Tree pandas DataFrame containing the filtered returns and purchases data, and thhe filtered main data.\n",
    "        \"\"\" \n",
    "\n",
    "        df_returns   = dataframe_raw.loc[dataframe_raw['Quantity']<0, ['CustomerID', 'Quantity']]\n",
    "\n",
    "        df_purchases = dataframe_raw.loc[dataframe_raw['Quantity'] >= 0, :]\n",
    "\n",
    "        # Filter main data\n",
    "        df_filtered  = keep_features( dataframe_raw, ['InvoiceNo' , 'StockCode', 'Quantity', 'InvoiceDate', 'UnitPrice', \n",
    "                                                      'CustomerID', 'Country'])\n",
    "        return df_filtered, df_purchases, df_returns\n",
    "\n",
    "    def run_date_preparation( dataframe_raw: pd.DataFrame) -> Tuple[ pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "             Descri\n",
    "                Preprocesses the input dataframe by performing column type conversion and filtering features\n",
    "             Args:\n",
    "                    dataframe_raw (pd.DataFrame): A pandas DataFrame containing raw sales data. \n",
    "             Returns:\n",
    "                 A tuple of three pandas DataFrame: df_filtered, df_purchases, and df_returns. \n",
    "                     df_filtered : A Dataframe containing the filtered main data\n",
    "                     df_purchases: A Dataframe containing the filtered purchases data\n",
    "                     df_returns  : A Dataframe containing the filtered returns date.\n",
    "         \"\"\"\n",
    "        change_column_type(dataframe_raw)\n",
    "        return filtering_features(dataframe_raw)\n",
    "    \n",
    "    data = pd.read_csv( dataset.path + \".csv\")\n",
    "    \n",
    "    df_filtered_inside, df_purchases_insider, df_returns_inside = run_data_preparation( data)\n",
    "    \n",
    "    df_filtererd_inside.to_csv( df_filtererd.path + \".csv\", index = False)\n",
    "    df_purchases_inside.to_csv( df_purchases.path + \".csv\", index = False)\n",
    "    df_returns_inside.  to_csv( df_returns.  path + \".csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ff93f3-8007-4548-8cff-cf3a009eb07a",
   "metadata": {},
   "source": [
    "# 3. Componente: Features engeneering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c23963f0-5a70-4a53-bc0c-5d924bf467c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component( packages_to_install = [\"pandas\", \"google.cloud.bigquery\", \"db.dtypes\", \"pandas.gbq\"], \n",
    "          base_image = \"python:3.10.6\",\n",
    "          output_component_file = \"features_engeneering_ecommerce.yaml\")\n",
    "\n",
    "def feature_engeneering( df_filtered: Input[Dataset],\n",
    "                        df_purchases: Input[Dataset],\n",
    "                        df_returns  : Input[Dataset]):\n",
    "    \n",
    "    from google.cloud import bigquery\n",
    "    from functools    import reduce\n",
    "    from typing       import Union\n",
    "    import pandas as pd\n",
    "    \n",
    "    def column_to_date( dataframe: pd.DataFrame, column_name: str, date_format: str = None) -> bool:\n",
    "        \"\"\"\n",
    "        Descrição:\n",
    "            Converte a coluna especificada em um datafreme para o tipo data.\n",
    "\n",
    "        Args: \n",
    "             dataframe(pd.DataFrame): O dataframe a ser processado.\n",
    "             column_name (str): O nome da coluna a ser convertida.\n",
    "        Returns:\n",
    "             bool: True se a conversão foi bem sucedida. False caso contrário.\n",
    "        \"\"\"\n",
    "        if column_name not in dataframe.columns:\n",
    "            raise ValueError(f'Coluna {column_name} não encontrado no dataframe de entrada')\n",
    "\n",
    "        try:  \n",
    "            if date_format:\n",
    "                dataframe[column_name] = pd.to_datetime(dataframe[column_name], format = date_format)\n",
    "            else:\n",
    "                dataframe[column_name] = pd.to_datetime(dataframe[column_name])\n",
    "        except (ValueError, TypeError):\n",
    "            raise ValueError(f'Could not convert column {column_name}')\n",
    "        # Retorna True se a conversão for bem sucedida\n",
    "        return True\n",
    "    \n",
    "    def keep_features(dataframe: pd.DataFrame, keep_columns: list) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Retorna um DataFrame com as colunas especificadas em keep_colunas\n",
    "\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): O DataFrame a ser processado.\n",
    "            keep_columns (list): A lista de nomes de colunas a serem mantidas no DataFrame resultante. \n",
    "        Returns:\n",
    "            pd.DataFrame: O DataFrame resultante com apenas as colunas especificadas em keep_columns.\n",
    "        \"\"\"\n",
    "        return dataframe[keep_columns]\n",
    "   \n",
    "    def calculate_gross_revenue(dataframe_purchases: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calcula a receita bruta de cada cliente com base nas colunas 'Quantity' e \n",
    "        'UnitPrice' e retorna um DataFrame com as colunas 'CustomerID' e 'gross_revenue'.\n",
    "        Args: \n",
    "            dataframe_purchases (pd.Dataframe): O DataFrame das compras contendo as colunas\n",
    "            'CustomerID', 'Quantity'\n",
    "        Returns:\n",
    "            pd.DataFrame: O DataFrame resultante contendo as colunas 'CustomerID' e 'grass revenue',\n",
    "        \"\"\"\n",
    "        # Verifica se as colunas necessárias estão presentes ou DataFrame de entrada\n",
    "        required_columns = {'CustomerID', 'Quantity', 'UnitPrice'}\n",
    "        missing_columns  = required_columns - set(dataframe_purchases.columns)\n",
    "        if missing_columns: \n",
    "            raise ValueError(f\"O DataFrame de entrada está faltando as sequintes colunas: {missing_columns}\")\n",
    "\n",
    "            # Calcula a receita bruta e agrupa por CustomerID\n",
    "        df = dataframe_purchases.copy()\n",
    "        df.loc[:, 'gross_revenue'] = df.loc[:, 'Quantity'] * df.loc[:, 'UnitPrice']\n",
    "        grouped_df = df.groupby('CustomerID').agg({'gross_revenue':'sum'}).reset_index().copy()\n",
    "\n",
    "        return grouped_df\n",
    "\n",
    "    def create_recency(dataframe_purchases: pd.DataFrame, dataframe_filtered: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calcula a recência da última compra para cada cliente.\n",
    "\n",
    "        Args:\n",
    "            dataframe_purchases(pd.DataFrame): DataFrame com as informações de compras de todos os clientes.\n",
    "            dataframe_filtered (pd.DataFrame): DataFrame filtrado apenas com as informaões dos clientes que des\n",
    "\n",
    "        Returns: \n",
    "            pd.DataFrame: DataFrame com as colunas 'CustomerID' e 'recency_days', indicando a recẽncia em dias de\n",
    "\n",
    "        \"\"\"\n",
    "        required_columns = {'CustomerID', 'InvoiceDate'}\n",
    "        missing_columns  = required_columns - set(dataframe_purchases.columns)\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"O DataFrame de entrada está faltando as seguintes colunas: {missing_columns}\")\n",
    "        # calcula a data da útima compra de cada cliente.\n",
    "        df_recency = dataframe_purchases.loc[:,['CustomerID', 'InvoiceDate']].groupby('CustomerID').max().reset_index()\n",
    "\n",
    "        # calcula a recência em dias da última compra de cada cliente em relação à data mais recente da base de\n",
    "        dataframe_filtered['InvoiceDate'] = pd.to_datetime( dataframe_filtered['InvoiceDate'])\n",
    "        df_recency['InvoiceDate']         = pd.to_datetime( df_recency['InvoiceDate'])\n",
    "        df_recency['recency_days'] = (dataframe_filtered['InvoiceDate'].max() - df_recency['InvoiceDate']).dt.days\n",
    "\n",
    "        # retorna o DataFrame apenas com as colunas 'CustomerID' e 'recency_days'\n",
    "        return df_recency[['CustomerID', 'recency_days']]\n",
    "    \n",
    "    def create_quantity_purchased( dataframe_purchases: pd.DataFrame ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calcula a quantidade de produtos adquiridos por cada cliente.\n",
    "\n",
    "        Args: \n",
    "            dataframe_purchases (pd.DataFrame): DataFrame com as informações de compras de todos os clientes.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame com as colunas 'CustomerID' e 'qty_products', indicando a quantidade de produtos\n",
    "\n",
    "        \"\"\"\n",
    "        required_columns = {'CustomerID', 'StockCode'}\n",
    "        missing_columns  = required_columns - set(dataframe_purchases.columns)\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"O DataFrame de entrada está faltando as seguintes colunas: {missing_columns}\")\n",
    "        #agrupa as informações de compras por 'CustomerID' e conta o número de StockCode para cada grupo\n",
    "        qty_purchased = dataframe_purchases.loc[:, ['CustomerID', 'StockCode']].groupby('CustomerID').count()\n",
    "\n",
    "        # renomeia o DataFrame com as colunas 'CustomerID' e 'qty_products'\n",
    "        qty_purchased = qty_purchased.reset_index().rename( columns={'StockCode': 'qty_products'})\n",
    "        #retorna a DataFrame com as coclunas 'CustomerID' e 'qty_products'\n",
    "        return qty_purchased \n",
    "    \n",
    "    def create_freq_purchases(dataframe_purchases: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calculates the purchase frequency of each customer based on the purchase history. \n",
    "\n",
    "        Parameters\n",
    "\n",
    "        ----------\n",
    "        pd.DataFrame\n",
    "            DataFrame with the purchase frequency of each customer, containing columns CustomerID and frequency\n",
    "        \"\"\"\n",
    "        required_columns = { 'CustomerID', 'InvoiceNo', 'InvoiceDate'}\n",
    "        missing_columns  = required_columns - set(dataframe_purchases.columns)\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"O DataFrame de entrada está faltando as seguintes colunas: {missing_columns}\")\n",
    "        # Calculate time range of purchases for each customer\n",
    "        df_aux = (dataframe_purchases[['CustomerID', 'InvoiceNo', 'InvoiceDate']]\n",
    "                  .drop_duplicates()\n",
    "                  .groupby('CustomerID')\n",
    "                  .agg(max_ = ('InvoiceDate', 'max'), \n",
    "                       min_ = ('InvoiceDate', 'min'),\n",
    "                       days_= ('InvoiceDate', lambda x: (( x.max() - x.min()).days) + 1),\n",
    "                       buy_ = ('InvoiceNo', 'count')).reset_index())\n",
    "\n",
    "        # Calculate frequency of purchases for each customer\n",
    "        df_aux['frequency'] = df_aux[['buy_', 'days_']].apply(\n",
    "            lambda x: x['buy_'] / x['days_'] if x['days_'] != 0 else 0, axis = 1)\n",
    "\n",
    "        return df_aux\n",
    "        \n",
    "    def create_qty_returns(dataframe_returns:pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Computes the total quantity of return products for each customer\n",
    "\n",
    "        Args:\n",
    "            dataframe_returns: A pandas DataFrame cocntaining information about returns.\n",
    "        Returns: \n",
    "            A pandas DataFrame with the total quantity of returned product for each customer.\n",
    "        \"\"\"\n",
    "        # Validate input data\n",
    "        if not all( col in dataframe_returns.columns for col in [ 'CustomerID', 'Quantity']):\n",
    "                raise ValueError(\"Input DataFrame must contain 'CustomerID' and 'Quantity' columns\")\n",
    "\n",
    "        # Compupte quantity of rerturns\n",
    "        df_returns = dataframe_returns[['CustomerID', 'Quantity']].groupby('CustomerID').sum().reset_index().rename(columns = {'Quantity':'qty_returns'})\n",
    "        df_returns['qty_returns'] = df_returns['qty_returns']* -1\n",
    "\n",
    "        return df_returns\n",
    "    \n",
    "    def run_feature_engineering( dataframe_filtered: pd.DataFrame, dataframe_purchases: pd.DataFrame, dataframe_returns: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Performs feature engineering on the input dataframes and returns a new dataframe with the engineered features\n",
    "\n",
    "        Args: \n",
    "            dataframe_filtered : A pandas DataFrame containing filtered customer order data.\n",
    "            dataframe_purchases: A pandas DataFrame containing customer purchase data. \n",
    "            dataframe_returns  : A pandas DataFrame containing customer return data.\n",
    "\n",
    "        Returns:\n",
    "            A pandas DataFrame with the engineered features for each customer.\n",
    "        \"\"\"\n",
    "        # Check if input dataframes are empty\n",
    "        if dataframe_filtered.empty:\n",
    "            raise ValueError(\"Input DataFrame 'dataframe_filtered' is empty\")\n",
    "        if dataframe_purchases.empty:\n",
    "            raise ValueError(\"Input DataFrame 'dataframe_purchases' is empty\")\n",
    "\n",
    "        # Check if required_columns are present in input daframes\n",
    "        required_columns = ['CustomerID', 'InvoiceDate', 'StockCode', 'Quantity', 'UnitPrice']\n",
    "        for df, name in zip( [dataframe_filtered, dataframe_purchases], ['dataframe_iltererd', 'dataframe_purchases']):\n",
    "            missing_columns = set( required_columns) - set(df.columns)\n",
    "            if missing_columns:\n",
    "                raise ValueError(f\"Missing_columns {missing_columns} in input DataFrame '{name}'\")\n",
    "            if 'CustomerID' not in dataframe_returns.columns:\n",
    "                raise ValueError( \"Column 'CustomerID' not found in input DataFrame 'dataframe_returns'\")\n",
    "            if 'Quantity' not in dataframe_returns.columns: \n",
    "                             raise ValueError(\"Column 'Quantity' not found in input DataFrame 'dataframe_returns'\")\n",
    "\n",
    "            # Perform feature engineering \n",
    "            df_fengi        = keep_features(dataframe_filtered,['CustomerID']).drop_duplicates( ignore_index = True)\n",
    "            gross_revenue   = calculate_gross_revenue(dataframe_purchases)\n",
    "            df_recency      = create_recency( dataframe_purchases, dataframe_filtered)\n",
    "            df_qty_products = create_quantity_purchased( dataframe_purchases)\n",
    "            df_freq         = create_freq_purchases( dataframe_purchases)\n",
    "            returns         = create_qty_returns( dataframe_returns)\n",
    "\n",
    "            # Merge dataframes\n",
    "            dfs             = [df_fengi, gross_revenue, df_recency, df_qty_products, df_freq, returns]\n",
    "            df_fengi        = reduce( lambda left,right: pd.merge( left, right, on = 'CustomerID', how = 'left'), dfs)\n",
    "\n",
    "            # Fil NaN values\n",
    "            df_fengi['qty_returns'] = df_fengi['qty_returns' ].fillna(0)\n",
    "\n",
    "            # Select final features and return dataframe\n",
    "            features        = ['CustomerID', 'gross_revenue','recency_days', 'qty_products', 'frequency', 'qty_returns'] \n",
    "\n",
    "            return keep_features( df_fengi, features).dropna()\n",
    "        \n",
    "    def save_to_bigquery(\n",
    "            dataframe         : pd.DataFrame, \n",
    "            project_name      : str, \n",
    "            dataset_table_name: str ): \n",
    "\n",
    "            client = bigquery.Client(project = project_name)\n",
    "\n",
    "            job = client.load_table_from_dataframe(dataframe, dataset_table_name)\n",
    "            job.result()\n",
    "         \n",
    "    def run_bq_query(sql: str, project_nameDataset: str) -> Union[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Description\n",
    "            Run a BigQuery query and return the job ID or result as a DataFrame\n",
    "        Args:\n",
    "            sql: SQL query, as a string, to exercate in BigQuery\n",
    "        Returns:\n",
    "            df: DataFrame of results from query, or error, if any \n",
    "        \"\"\"\n",
    "        bq_client  = bigquery.Client(project = project_name)\n",
    "        \n",
    "        # Try \n",
    "        job_config = bigquery.QueryJobConfig( dry_run = True, use_query_cache = False)\n",
    "        bq_client.query(sql, job_config = job_config)\n",
    "        \n",
    "        # If dry run succes without errors, ALGUMA to run query\n",
    "        job_config   = bigquery.QueryjobConfig()\n",
    "        client_result= bq_client.query(sql, job_config = job_config)\n",
    "        \n",
    "        job_id = client_result.job_id\n",
    "        \n",
    "        # _for_query /too to finish runnig get return data frame\n",
    "        df = client_result.result().to_arrow().to_pandas()\n",
    "        print(f\"Finished job id { job_id}\")\n",
    "        return df\n",
    "        \n",
    "    df_filtered  = pd.read_csv( df_filtered.path + '.csv')\n",
    "    df_purchases = pd.read_csv( df_purchases.path + '.csv')\n",
    "    df_returns   = pd.read_csv( df_returns.pah + '.csv')   \n",
    "    \n",
    "    column_to_date( df_filtered, \"InvoiceDate\")\n",
    "    column_to_date( df_purchases, \"InvoiceDate\")\n",
    "    df_fengi  = run_feature_engineering( df_filtered, df_purchases, df_returns)\n",
    "    \n",
    "    query = \"\"\"CREATE OR REPLACE TABLE 'braided-period-442813-v0.ecommerce_cluster_insider.dados_engenharia_features' as\n",
    "            (   SELECT\n",
    "                    *,\n",
    "                    generate_uuid() as values, \n",
    "                    current_timestamp() as timestamp, \n",
    "                FROM \n",
    "                    'braided-period-442813-v0.ecommerce_cluster_insider.dados_engenharia_features')\n",
    "                \"\"\"\n",
    "    run_bq_query(query, project_name = 'braided-period-442813-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5595212-c3c7-4c46-9894-f3c427076a94",
   "metadata": {},
   "source": [
    "# Salvando no próprio bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3768129-e18b-4126-a3aa-f050d8f98ccf",
   "metadata": {},
   "source": [
    "## 1. Componente: Captura dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e1400b8-2a7b-4126-87b0-f1e640caeba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos criar e instalar alguns components\n",
    "@component( packages_to_install = [\"pandas\", \"google.cloud.bigquery\", \"db.dtypes\", \"pandas.gbq\"], \n",
    "          base_image = \"python:3.10.6\",\n",
    "          output_component_file = \"captura_dados_ecommerce.yaml\")\n",
    "\n",
    "def captura_dados():\n",
    "    import os \n",
    "    import pandas as pd\n",
    "    import logging\n",
    "    from google.cloud import bigquery\n",
    "    \n",
    "    PROJECT_ID   = \"braided-period-442813-v0\" # O Bucket onde os dados estão sendo amazenado\n",
    "    DATASET_ID   = \"gcp_bq\" # Nome ppara os dados gerados \n",
    "    project_number = os.enviros['CLOUD_NO_PROJECT_ID']\n",
    "    TABLE_RAW_ID = \"dados_ecommerce_raw\"\n",
    "    TABLE_ID     = \"ecommerce_cds\"\n",
    "    \n",
    "    def run_bq_query(sql: str, project_name: str) -> Union[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Description\n",
    "            Run a BigQuery query and return the job ID or result as a DataFrame\n",
    "        Args:\n",
    "            sql: SQL query, as a string, to exercate in BigQuery\n",
    "        Returns:\n",
    "            df: DataFrame of results from query, or error, if any \n",
    "        \"\"\"\n",
    "        bq_client  = bigquery.Client(project = project_name)\n",
    "        \n",
    "        # Try dry run before executing query to catch any errors\n",
    "        job_config = bigquery.QueryJobConfig( dry_run = True, use_query_cache = False)\n",
    "        bq_client.query(sql, job_config = job_config)\n",
    "        \n",
    "        # If dry run succes without errors, ALGUMA to run query\n",
    "        job_config   = bigquery.QueryjobConfig()\n",
    "        client_result= bq_client.query(sql, job_config = job_config)\n",
    "        \n",
    "        job_id = client_result.job_id\n",
    "        \n",
    "        # wait for query/job to finish runnig, then get return data frame\n",
    "        df = client_result.result().to_arrow().to_pandas()\n",
    "        print(f\"Finished job id { job_id}\")\n",
    "        return df\n",
    "    \n",
    "    query = f\"\"\"\n",
    "                CREATE OR REPLACE TABLE\n",
    "                '{PROJECT_ID},{DATASET_ID}, {TABLE_ID}' (InvoiceNo STRING,\n",
    "                StockCode STRING, \n",
    "                Description STRING,\n",
    "                Quantity INT64, \n",
    "                InvoiceDate DATE, \n",
    "                UnitPrice FLAT64, \n",
    "                CustomerID FLOAT64, \n",
    "                CustomerID FLOAT64,\n",
    "                Country STRING)\n",
    "                PARTITION BY\n",
    "                    InvoiceDate AS ( \n",
    "                    WITH \n",
    "                        not_nulls AS ( \n",
    "                        SELECT\n",
    "                            *\n",
    "                        FROM \n",
    "                            '{PROJECT_ID}, {DATASET_ID}, {TABLE_RAW_ID}'\n",
    "                        WHERE\n",
    "                            InvoiceDate <= CURRENT_DATE()\n",
    "                            AND CustomerID IS NOT NULL\n",
    "                            AND Description IS NOT NULL),\n",
    "                            filtering_features AS (\n",
    "                            SELECT\n",
    "                                *\n",
    "                            FROM \n",
    "                                not_nulls\n",
    "                            WHERE\n",
    "                                UnitPrice >= 0.04\n",
    "                                AND Country NOT IN ('European Comunity', 'Unspecified')\n",
    "                                AND StockCode NOT IN ( 'POST',\n",
    "                                'D',\n",
    "                                'DOT,'\n",
    "                                'M', \n",
    "                                'S', \n",
    "                                'AMAZONFEE',\n",
    "                                'm',\n",
    "                                'DCGSSBOY', \n",
    "                                'DCGSSGIRL',\n",
    "                                'PADS', 'B', 'CRUK')\n",
    "                                AND CustomerID != 16446)\n",
    "                                SELECT \n",
    "                                    * \n",
    "                                FROM\n",
    "                                    filtering_features );\n",
    "                \"\"\"\n",
    "    run_bq_query( query, project_name = PROJECT_ID)\n",
    "    logging.info(f\"Tabela criada: {PROJECT_ID}, {DATASET_ID}, {TABLE_ID}\") # informante"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250dc019-f3c4-425c-bfbb-b46656fee1c5",
   "metadata": {},
   "source": [
    "## 2. Componente: Preparação dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84b1e0e0-cc01-4bfd-8d52-3ab1d816044c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component( packages_to_install = [\"pandas\", \"google.cloud.bigquery\", \"db.dtypes\", \"pandas.gbq\"], \n",
    "                     base_image = \"python:3.10.6\",\n",
    "           output_component_file= \"data_preparation_ecommerce.yaml\")\n",
    "def data_preparation( ):\n",
    "    from typing import Tuple\n",
    "    import os\n",
    "    import logging\n",
    "    import pandas as pd\n",
    "    import logging\n",
    "    from google.cloud import bigquery\n",
    "    import pandas_gbp\n",
    "    \n",
    "    logging_info('Iniciando o componente')\n",
    "    \n",
    "    PROJECT_ID     = 'braided-period-442813-v0'\n",
    "    DATASET_ID     = 'gcp_bq'\n",
    "    TABLE_RAW_ID   = 'dados_ecommerce_raw'\n",
    "    TABLE_ID       = 'ecommerce_cds'\n",
    "    TABLE_FILTERED_TEMP_ID  = 'temp_data_filtered'\n",
    "    TABLE_PURCHASES_TEMP_ID = 'temp_data_purchases'\n",
    "    TABLE_RETURNS_TEMP_ID   = 'temp_data_returns'\n",
    "    PROJECT_NUMBER          = os.environ[\"CLOUD_ML_PROJECT_ID\"]\n",
    "    \n",
    "    def keep_features(dataframe: pd.DataFrame, keep_columns: list) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Retorna um DataFrame com as colunas especificadas em keep_colunas\n",
    "\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): O DataFrame a ser processado.\n",
    "            keep_columns (list): A lista de nomes de colunas a serem mantidas no DataFrame resultante. \n",
    "        Returns:\n",
    "            pd.DataFrame: O DataFrame resultante com apenas as colunas especificadas em keep_columns.\n",
    "        \"\"\"\n",
    "    return dataframe[keep_columns]\n",
    "    \n",
    "    def column_to_init( dataframe:pd.DataFrame, column_name:str) -> bool:\n",
    "        \"\"\"\n",
    "            Descrição:\n",
    "                Converte a coluna especificada em um datafreme parao tipo int\n",
    "\n",
    "            Args: \n",
    "                 dataframe(pd.DataFrame): O dataframe a ser processado.\n",
    "                 column_name (str): O nome da coluna a ser convertida.\n",
    "            Returns:\n",
    "                 bool: True se a conversão foi bem sucedida. False caso contrário.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            dataframe[column_name] = dataframe[column_name].astype(int)\n",
    "        except (ValueError, typerError):\n",
    "            return False\n",
    "            # Lídar com valores ausentes e conversões inválidas\n",
    "            \n",
    "        #if column_name not in dataframe.columns:\n",
    "         #   raise ValueError(f'Coluna {column_name} não encontrado')\n",
    "        #try:    \n",
    "        #    dataframe[column_name] = dataframe[column_name].astype(int)\n",
    "        #except:\n",
    "        #    raise ValueError(f'Não foi possível converter a coluna {column_name} para inteiro.')\n",
    "        return True\n",
    "    \n",
    "    def column_to_date( dataframe: pd.DataFrame, column_name: str, date_format: str = None) -> bool:\n",
    "        \"\"\"\n",
    "        Descrição:\n",
    "            Converte a coluna especificada em um datafreme parao tipo data\n",
    "\n",
    "        Args: \n",
    "             dataframe(pd.DataFrame): O dataframe a ser processado.\n",
    "             column_name (str): O nome da coluna a ser convertida.\n",
    "        Returns:\n",
    "             bool: True se a conversão foi bem sucedida. False caso contrário.\n",
    "        \"\"\"\n",
    "        if column_name not in dataframe.columns:\n",
    "            raise ValueError(f'Coluna {column_name} não encontrado no dataframe de entrada')\n",
    "\n",
    "        try:  \n",
    "            if date_format:\n",
    "                dataframe[column_name] = pd.to_datetime(dataframe[column_name], format = date_format)\n",
    "            else:\n",
    "                dataframe[column_name] = pd.to_datetime(dataframe[column_name])\n",
    "        except (ValueError, TypeError):\n",
    "            raise ValueError(f'Could not convert column {column_name}')\n",
    "        # Retorna True se a conversão for bem sucedida\n",
    "        return True\n",
    "\n",
    "\n",
    "    def change_column_type(dataframe_raw: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Changes the data type of column g in a DataFrame\n",
    "\n",
    "        Args:\n",
    "            dataframe_raw: A pandas DataFrame\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        column_to_init( dataframe_raw, 'CustomerID')\n",
    "        column_to_date( dataframe_raw, 'InvoiceDate')\n",
    "\n",
    "    def filterig_features(dataframe_raw: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Descrição\n",
    "            Filters and preprocesses the input Dataframe.\n",
    "\n",
    "        Args:\n",
    "            dataframe_raw: A pandas Dataframe containing_raw sales data.\n",
    "        Retornos: \n",
    "            Tree pandas DataFrame containing the filtered returns and purchases data, and thhe filtered main data.\n",
    "        \"\"\" \n",
    "\n",
    "        df_returns   = dataframe_raw.loc[dataframe_raw['Quantity']<0, ['CustomerID', 'Quantity']]\n",
    "\n",
    "        df_purchases = dataframe_raw.loc[dataframe_raw['Quantity'] >= 0, :]\n",
    "\n",
    "        # Filter main data\n",
    "        df_filtered  = keep_features( dataframe_raw, ['InvoiceNo' , 'StockCode', 'Quantity', 'InvoiceDate', 'UnitPrice', \n",
    "                                                      'CustomerID', 'Country'])\n",
    "        return df_filtered, df_purchases, df_returns\n",
    "\n",
    "    def run_date_preparation( dataframe_raw: pd.DataFrame) -> Tuple[ pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "             Descri\n",
    "                Preprocesses the input dataframe by performing column type conversion and filtering features\n",
    "             Args:\n",
    "                    dataframe_raw (pd.DataFrame): A pandas DataFrame containing raw sales data. \n",
    "             Returns:\n",
    "                 A tuple of three pandas DataFrame: df_filtered, df_purchases, and df_returns. \n",
    "                     df_filtered : A Dataframe containing the filtered main data\n",
    "                     df_purchases: A Dataframe containing the filtered purchases data\n",
    "                     df_returns  : A Dataframe containing the filtered returns date.\n",
    "         \"\"\"\n",
    "        change_column_type(dataframe_raw)\n",
    "        return filtering_features(dataframe_raw)\n",
    "    query_sql = f\"\"\"\" SELECT * \n",
    "    FROM '{PROJECT_ID}, {DATASET_ID},{TABLE_ID}'\n",
    "    WHERE InvoiceDATE <= CURRENT_DATE \"\"\"\n",
    "    \n",
    "    data = pd.read_gbq(query = query_sql,\n",
    "                       project_id = PROJECT_NUMBER)\n",
    "    logging.info( f\"Tabela carregada: '{PROJECT_ID},{DATASET_ID}, {TABLE_ID}'\")\n",
    "    \n",
    "    df_filtered, df_purchases, df_returns = run_data_preparation( data)\n",
    "    \n",
    "    pandas_gbq.to_gbq(df_filtererd, f'{PROJECT_ID}, {DATASET_ID}, {TABLE_FILTERED_TEMP_ID}', project_id = PROJECT_NUMBER, if_exists = 'replace')\n",
    "    pandas_gbq.to_gbq( df_purchases,f'{PROJECT_ID}, {DATASET_ID},{TABLE_PURCHASES_TEMP_ID}', project_id = PROJECT_NUMBER, if_exists = 'replace')\n",
    "    pandas_gbq.to_gbq( df_returns,  f'{PROJECT_ID}, {DATASET_ID},{TABLE_RETURNS_TEMP_ID}', project_id = PROJECT_NUMBER, if_exists = 'replace')\n",
    "    \n",
    "    logging.info(f'Tablelas criadas no BigQuery: {TABLE_FILTERED_TEMP_ID} e {TABLE_PURCHASES_TEMP_ID} e {TABLE_RETURNS_TEMP_ID}') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429efa3e-0435-4581-9072-3cf31b8a717f",
   "metadata": {},
   "source": [
    "## 3. Componente: Features engeneering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6992b20-6e95-4037-ba48-888f99ce3948",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component( packages_to_install = [\"pandas\", \"google.cloud.bigquery\", \"db.dtypes\", \"pandas.gbq\", \"google-cloud\"], \n",
    "          base_image = \"python:3.10.6\",\n",
    "          output_component_file = \"features_engeneering_ecommerce.yaml\")\n",
    "\n",
    "def feature_engeneering( df_filtered: Input[Dataset],\n",
    "                        df_purchases: Input[Dataset],\n",
    "                        df_returns  : Input[Dataset]):\n",
    "    \n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud.exceptions import NotFound\n",
    "    import pandas as pd\n",
    "    import pandas_gbq\n",
    "    import os\n",
    "    from functools    import reduce\n",
    "    from typing       import Union\n",
    "    \n",
    "    PROJECT_ID     = 'braided-period-442813-v0'\n",
    "    DATASET_ID     = 'gcp_bq'\n",
    "    TABLE_ID       = 'dados_engenharia_features'\n",
    "    TABLE_RAW_ID   = 'dados_ecommerce_raw'\n",
    "    TABLE_FILTERED_TEMP_ID  = 'temp_data_filtered'\n",
    "    TABLE_PURCHASES_TEMP_ID = 'temp_data_purchases'\n",
    "    TABLE_RETURNS_TEMP_ID   = 'temp_data_returns'\n",
    "    PROJECT_NUMBER          = os.environ[\"CLOUD_ML_PROJECT_ID\"]\n",
    "    \n",
    "    logging.info('Iniciando o componente')\n",
    "    \n",
    "    def column_to_date( dataframe: pd.DataFrame, column_name: str, date_format: str = None) -> bool:\n",
    "        \"\"\"\n",
    "        Descrição:\n",
    "            Converte a coluna especificada em um datafreme para o tipo data.\n",
    "\n",
    "        Args: \n",
    "             dataframe(pd.DataFrame): O dataframe a ser processado.\n",
    "             column_name (str): O nome da coluna a ser convertida.\n",
    "        Returns:\n",
    "             bool: True se a conversão foi bem sucedida. False caso contrário.\n",
    "        \"\"\"\n",
    "        if column_name not in dataframe.columns:\n",
    "            raise ValueError(f'Coluna {column_name} não encontrado no dataframe de entrada')\n",
    "\n",
    "        try:  \n",
    "            if date_format:\n",
    "                dataframe[column_name] = pd.to_datetime(dataframe[column_name], format = date_format)\n",
    "            else:\n",
    "                dataframe[column_name] = pd.to_datetime(dataframe[column_name])\n",
    "        except (ValueError, TypeError):\n",
    "            raise ValueError(f'Could not convert column {column_name}')\n",
    "        # Retorna True se a conversão for bem sucedida\n",
    "        return True\n",
    "    \n",
    "    def keep_features(dataframe: pd.DataFrame, keep_columns: list) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Retorna um DataFrame com as colunas especificadas em keep_colunas\n",
    "\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): O DataFrame a ser processado.\n",
    "            keep_columns (list): A lista de nomes de colunas a serem mantidas no DataFrame resultante. \n",
    "        Returns:\n",
    "            pd.DataFrame: O DataFrame resultante com apenas as colunas especificadas em keep_columns.\n",
    "        \"\"\"\n",
    "        return dataframe[keep_columns]\n",
    "   \n",
    "    def calculate_gross_revenue(dataframe_purchases: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calcula a receita bruta de cada cliente com base nas colunas 'Quantity' e \n",
    "        'UnitPrice' e retorna um DataFrame com as colunas 'CustomerID' e 'gross_revenue'.\n",
    "        Args: \n",
    "            dataframe_purchases (pd.Dataframe): O DataFrame das compras contendo as colunas\n",
    "            'CustomerID', 'Quantity'\n",
    "        Returns:\n",
    "            pd.DataFrame: O DataFrame resultante contendo as colunas 'CustomerID' e 'grass revenue',\n",
    "        \"\"\"\n",
    "        # Verifica se as colunas necessárias estão presentes ou DataFrame de entrada\n",
    "        required_columns = {'CustomerID', 'Quantity', 'UnitPrice'}\n",
    "        missing_columns  = required_columns - set(dataframe_purchases.columns)\n",
    "        if missing_columns: \n",
    "            raise ValueError(f\"O DataFrame de entrada está faltando as sequintes colunas: {missing_columns}\")\n",
    "\n",
    "            # Calcula a receita bruta e agrupa por CustomerID\n",
    "        df = dataframe_purchases.copy()\n",
    "        df.loc[:, 'gross_revenue'] = df.loc[:, 'Quantity'] * df.loc[:, 'UnitPrice']\n",
    "        grouped_df = df.groupby('CustomerID').agg({'gross_revenue':'sum'}).reset_index().copy()\n",
    "\n",
    "        return grouped_df\n",
    "\n",
    "    def create_recency(dataframe_purchases: pd.DataFrame, dataframe_filtered: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calcula a recência da última compra para cada cliente.\n",
    "\n",
    "        Args:\n",
    "            dataframe_purchases(pd.DataFrame): DataFrame com as informações de compras de todos os clientes.\n",
    "            dataframe_filtered (pd.DataFrame): DataFrame filtrado apenas com as informaões dos clientes que des\n",
    "\n",
    "        Returns: \n",
    "            pd.DataFrame: DataFrame com as colunas 'CustomerID' e 'recency_days', indicando a recẽncia em dias de\n",
    "\n",
    "        \"\"\"\n",
    "        required_columns = {'CustomerID', 'InvoiceDate'}\n",
    "        missing_columns  = required_columns - set(dataframe_purchases.columns)\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"O DataFrame de entrada está faltando as seguintes colunas: {missing_columns}\")\n",
    "        # calcula a data da útima compra de cada cliente.\n",
    "        df_recency = dataframe_purchases.loc[:,['CustomerID', 'InvoiceDate']].groupby('CustomerID').max().reset_index()\n",
    "\n",
    "        # calcula a recência em dias da última compra de cada cliente em relação à data mais recente da base de\n",
    "        dataframe_filtered['InvoiceDate'] = pd.to_datetime( dataframe_filtered['InvoiceDate'])\n",
    "        df_recency['InvoiceDate']         = pd.to_datetime( df_recency['InvoiceDate'])\n",
    "        df_recency['recency_days'] = (dataframe_filtered['InvoiceDate'].max() - df_recency['InvoiceDate']).dt.days\n",
    "\n",
    "        # retorna o DataFrame apenas com as colunas 'CustomerID' e 'recency_days'\n",
    "        return df_recency[['CustomerID', 'recency_days']]\n",
    "    \n",
    "    def create_quantity_purchased( dataframe_purchases: pd.DataFrame ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calcula a quantidade de produtos adquiridos por cada cliente.\n",
    "\n",
    "        Args: \n",
    "            dataframe_purchases (pd.DataFrame): DataFrame com as informações de compras de todos os clientes.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame com as colunas 'CustomerID' e 'qty_products', indicando a quantidade de produtos\n",
    "\n",
    "        \"\"\"\n",
    "        required_columns = {'CustomerID', 'StockCode'}\n",
    "        missing_columns  = required_columns - set(dataframe_purchases.columns)\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"O DataFrame de entrada está faltando as seguintes colunas: {missing_columns}\")\n",
    "        #agrupa as informações de compras por 'CustomerID' e conta o número de StockCode para cada grupo\n",
    "        qty_purchased = dataframe_purchases.loc[:, ['CustomerID', 'StockCode']].groupby('CustomerID').count()\n",
    "\n",
    "        # renomeia o DataFrame com as colunas 'CustomerID' e 'qty_products'\n",
    "        qty_purchased = qty_purchased.reset_index().rename( columns={'StockCode': 'qty_products'})\n",
    "        #retorna a DataFrame com as coclunas 'CustomerID' e 'qty_products'\n",
    "        return qty_purchased \n",
    "    \n",
    "    def create_freq_purchases(dataframe_purchases: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calculates the purchase frequency of each customer based on the purchase history. \n",
    "\n",
    "        Parameters\n",
    "\n",
    "        ----------\n",
    "        pd.DataFrame\n",
    "            DataFrame with the purchase frequency of each customer, containing columns CustomerID and frequency\n",
    "        \"\"\"\n",
    "        required_columns = { 'CustomerID', 'InvoiceNo', 'InvoiceDate'}\n",
    "        missing_columns  = required_columns - set(dataframe_purchases.columns)\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"O DataFrame de entrada está faltando as seguintes colunas: {missing_columns}\")\n",
    "        # Calculate time range of purchases for each customer\n",
    "        df_aux = (dataframe_purchases[['CustomerID', 'InvoiceNo', 'InvoiceDate']]\n",
    "                  .drop_duplicates()\n",
    "                  .groupby('CustomerID')\n",
    "                  .agg(max_ = ('InvoiceDate', 'max'), \n",
    "                       min_ = ('InvoiceDate', 'min'),\n",
    "                       days_= ('InvoiceDate', lambda x: (( x.max() - x.min()).days) + 1),\n",
    "                       buy_ = ('InvoiceNo', 'count')).reset_index())\n",
    "\n",
    "        # Calculate frequency of purchases for each customer\n",
    "        df_aux['frequency'] = df_aux[['buy_', 'days_']].apply(\n",
    "            lambda x: x['buy_'] / x['days_'] if x['days_'] != 0 else 0, axis = 1)\n",
    "\n",
    "        return df_aux\n",
    "        \n",
    "    def create_qty_returns(dataframe_returns:pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Computes the total quantity of return products for each customer\n",
    "\n",
    "        Args:\n",
    "            dataframe_returns: A pandas DataFrame cocntaining information about returns.\n",
    "        Returns: \n",
    "            A pandas DataFrame with the total quantity of returned product for each customer.\n",
    "        \"\"\"\n",
    "        # Validate input data\n",
    "        if not all( col in dataframe_returns.columns for col in [ 'CustomerID', 'Quantity']):\n",
    "                raise ValueError(\"Input DataFrame must contain 'CustomerID' and 'Quantity' columns\")\n",
    "\n",
    "        # Compupte quantity of rerturns\n",
    "        df_returns = dataframe_returns[['CustomerID', 'Quantity']].groupby('CustomerID').sum().reset_index().rename(columns = {'Quantity':'qty_returns'})\n",
    "        df_returns['qty_returns'] = df_returns['qty_returns']* -1\n",
    "\n",
    "        return df_returns\n",
    "    \n",
    "    def run_feature_engineering( dataframe_filtered: pd.DataFrame, dataframe_purchases: pd.DataFrame, dataframe_returns: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Performs feature engineering on the input dataframes and returns a new dataframe with the engineered features\n",
    "\n",
    "        Args: \n",
    "            dataframe_filtered : A pandas DataFrame containing filtered customer order data.\n",
    "            dataframe_purchases: A pandas DataFrame containing customer purchase data. \n",
    "            dataframe_returns  : A pandas DataFrame containing customer return data.\n",
    "\n",
    "        Returns:\n",
    "            A pandas DataFrame with the engineered features for each customer.\n",
    "        \"\"\"\n",
    "        # Check if input dataframes are empty\n",
    "        if dataframe_filtered.empty:\n",
    "            raise ValueError(\"Input DataFrame 'dataframe_filtered' is empty\")\n",
    "        if dataframe_purchases.empty:\n",
    "            raise ValueError(\"Input DataFrame 'dataframe_purchases' is empty\")\n",
    "\n",
    "        # Check if required_columns are present in input daframes\n",
    "        required_columns = ['CustomerID', 'InvoiceDate', 'StockCode', 'Quantity', 'UnitPrice']\n",
    "        for df, name in zip( [dataframe_filtered, dataframe_purchases], ['dataframe_iltererd', 'dataframe_purchases']):\n",
    "            missing_columns = set( required_columns) - set(df.columns)\n",
    "            if missing_columns:\n",
    "                raise ValueError(f\"Missing_columns {missing_columns} in input DataFrame '{name}'\")\n",
    "            if 'CustomerID' not in dataframe_returns.columns:\n",
    "                raise ValueError( \"Column 'CustomerID' not found in input DataFrame 'dataframe_returns'\")\n",
    "            if 'Quantity' not in dataframe_returns.columns: \n",
    "                             raise ValueError(\"Column 'Quantity' not found in input DataFrame 'dataframe_returns'\")\n",
    "\n",
    "            # Perform feature engineering \n",
    "            df_fengi        = keep_features(dataframe_filtered,['CustomerID']).drop_duplicates( ignore_index = True)\n",
    "            gross_revenue   = calculate_gross_revenue(dataframe_purchases)\n",
    "            df_recency      = create_recency( dataframe_purchases, dataframe_filtered)\n",
    "            df_qty_products = create_quantity_purchased( dataframe_purchases)\n",
    "            df_freq         = create_freq_purchases( dataframe_purchases)\n",
    "            returns         = create_qty_returns( dataframe_returns)\n",
    "\n",
    "            # Merge dataframes\n",
    "            dfs             = [df_fengi, gross_revenue, df_recency, df_qty_products, df_freq, returns]\n",
    "            df_fengi        = reduce( lambda left,right: pd.merge( left, right, on = 'CustomerID', how = 'left'), dfs)\n",
    "\n",
    "            # Fil NaN values\n",
    "            df_fengi['qty_returns'] = df_fengi['qty_returns' ].fillna(0)\n",
    "\n",
    "            # Select final features and return dataframe\n",
    "            features        = ['CustomerID', 'gross_revenue','recency_days', 'qty_products', 'frequency', 'qty_returns'] \n",
    "\n",
    "            return keep_features( df_fengi, features).dropna()\n",
    "        \n",
    "    def save_to_bigquery(\n",
    "            dataframe         : pd.DataFrame, \n",
    "            project_name      : str, \n",
    "            dataset_table_name: str ): \n",
    "\n",
    "            client = bigquery.Client(project = project_name)\n",
    "\n",
    "            job = client.load_table_from_dataframe(dataframe, dataset_table_name)\n",
    "            job.result()\n",
    "         \n",
    "    def run_bq_query(sql: str, project_name: str) -> Union[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Description\n",
    "            Run a BigQuery query and return the job ID or result as a DataFrame\n",
    "        Args:\n",
    "            sql: SQL query, as a string, to exercate in BigQuery\n",
    "        Returns:\n",
    "            df: DataFrame of results from query, or error, if any \n",
    "        \"\"\"\n",
    "        bq_client  = bigquery.Client(project = project_name)\n",
    "        \n",
    "        # Try dry run before executing query to catch any errors\n",
    "        job_config = bigquery.QueryJobConfig( dry_run = True, use_query_cache = False)\n",
    "        bq_client.query(sql, job_config = job_config)\n",
    "        \n",
    "        # If dry run succes without errors, ALGUMA to run query\n",
    "        job_config   = bigquery.QueryjobConfig()\n",
    "        client_result= bq_client.query(sql, job_config = job_config)\n",
    "        \n",
    "        job_id = client_result.job_id\n",
    "        \n",
    "        # _for_query /too to finish runnig get return data frame\n",
    "        df = client_result.result().to_arrow().to_pandas()\n",
    "        print(f\"Finished job id { job_id}\")\n",
    "        return df\n",
    "    \n",
    "    logging.info(\"Carregando as tabelas da preparação de dados\")\n",
    "    query_filtered = f\"\"\" SELECT * \n",
    "                    FROM '{PROJECT_ID}, {DATASET_ID},{TABLE_FILTERED_TEMP_ID}'\n",
    "                    WHERE InvoiceDate <= CURRENT_TIMESTAMP() \"\"\"\n",
    "    \n",
    "    df_filtered  = pd.read_gbq( query_filtered, project_id = PROJECT_NUMBER)\n",
    "    \n",
    "    query_purchases = f\"\"\" SELECT * \n",
    "                    FROM '{PROJECT_ID}, {DATASET_ID},{TABLE_PURCHASES_TEMP_ID}'\n",
    "                    WHERE InvoiceDate <= CURRENT_TIMESTAMP() \"\"\"\n",
    "    \n",
    "    df_purchases = pd.read_gbq(query = query_purchases, project_id = PROJECT_NUMBER)\n",
    "    \n",
    "    query_returns =  f\"\"\" SELECT * \n",
    "                    FROM '{PROJECT_ID}, {DATASET_ID}, {TABLE_RETURNS_TEMP_ID}' \"\"\"\n",
    "                    \n",
    " \n",
    "    df_returns   = pd.read_gqb( query_returns, project_id = PROJECT_NUMBER)   \n",
    "    \n",
    "    logging.info('Transformando a coluna InvoiceDate para o tipo DATE')\n",
    "    column_to_date(df_filtered, 'InvoiceDate')\n",
    "    column_to_date(df_purchases, 'InvoiceDate')\n",
    "    \n",
    "    logging.info(f' Iniciando a verificação de existencia da tabela: {DATASET_ID},{TABLE_ID}')\n",
    "    \n",
    "    # Verifica se a tabela existe\n",
    "    if table_exists(f\"{PROJECT_ID},{DATASET_ID},{TABLE_ID}\"):\n",
    "        logging.info(\"Tabela existente, inicia inserrção de novos dados\")\n",
    "    \n",
    "        sql_new_customers = f\"\"\"SELECT\n",
    "                                DISTINCT CustomerID\n",
    "                                FROM \n",
    "                                    '{PROJECT_ID}, {DATASET_ID},{TABLE_RAW_ID}'\n",
    "                                WHERE \n",
    "                                    InvoiceDate = CURRENT_DATE()\"\"\"\n",
    "        new_customers = pd.read_gbq(sql_new_customers, project_id = PROJECT_NUMBER)['CustomerID'].tolist()\n",
    "        \n",
    "                   \n",
    "        df_fengi  = run_feature_engineering( df_filtered.loc[df_filtered['CustomerID'].isin(new_customers)], \n",
    "                                             df_purchases.loc[df_purchases['CustomerID'].isin(new_customers)], \n",
    "                                             df_returns.loc[df_returns['CustomerID'].isin(new_customers)])\n",
    "        # Inserir os dados na tabela usando SQL\n",
    "        pandas_gbq.to_gbq(df_forgi, f'{PROJECT_ID}, {DATASET_TO}, {TABLE_ID}', project_id = PROJECT_NUMBER, if_exists = 'append')\n",
    "        sql_update_new_customer = f\"\"\" \n",
    "                                UPDATE '{PROJECT_ID}, {DATASET_ID}, {TABLE_ID}'\n",
    "                                SET values = generate_uuid(),\n",
    "                                timestamp  = current_timestamp()\n",
    "                                WHERE CustomerID IN {tuple(new_customers)}\"\"\"\n",
    "        loggin.info( sql_update_new_customer)\n",
    "        run_bq_query(sql_update_new_customer, project_name = PROJECT_ID)\n",
    "    else:\n",
    "        # Cria a tabela e insere os dados\n",
    "        logging.info('Tabela não existente, cria a tabela e inicia inserção dos dados')\n",
    "        df_fengi = run_feature_engineering( df_filtered, df_purchases, df_returns)                                                   \n",
    "        pandas_gbq.to_gbq(df_fengi, f'{PROJECT_ID}, {DATASET_ID}, {TABLE_ID}', project_id = PROJECT_NUMBER, if_exists = 'fail')\n",
    "        query = f\"\"\"\n",
    "                CREATE OR REPLACE TABLE  '{PROJECT_ID}, {DATASET_ID}, {TABLE_ID}' as\n",
    "            (   SELECT\n",
    "                    *,\n",
    "                    generate_uuid() as values, \n",
    "                    current_timestamp() as timestamp, \n",
    "                FROM \n",
    "                    '{PROJECT_ID}, {DATASET_ID}, {TABLE_ID}');\n",
    "                \"\"\"\n",
    "        run_bq_query(query, project_name = PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cd3373-473e-4e89-aa0e-0fb85d3d2802",
   "metadata": {},
   "source": [
    "## 4. Componente: Feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "108299fd-713b-4e6b-97fc-a92494616d73",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4081110085.py, line 52)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[18], line 52\u001b[0;36m\u001b[0m\n\u001b[0;31m    values_feature_configs = [\"gross_revenue\": { \"value_type\" : \"DOUBLE\",\u001b[0m\n\u001b[0m                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "@component( packages_to_install = [\"pandas\", \"google.cloud.aiplatform\", \"pyarrow\"], \n",
    "          base_image = \"python:3.10.6\",\n",
    "          output_component_file = \"features_storo.yaml\")\n",
    "\n",
    "def create_feature_store():\n",
    "    import os\n",
    "    import logging \n",
    "    #from config import project_id\n",
    "    from google.cloud import aiplatform\n",
    "    from google.cloud.aiplatform import Feature, Featurestore\n",
    "    # https://medium.com/google-cloud/how-do-you-use-feature-store-in-the-mlops-process-on-vertex-ai-802ddca2cac4\n",
    "    # https://www.youtube.com/watch?v=jXD85fxAhvQ\n",
    "    \n",
    "    import pandas as pd\n",
    "    import pandas_gbq\n",
    "    \n",
    "    logging.info('Iniciando o componente')\n",
    "    \n",
    "    PROJECT_ID     = 'braided-period-442813-v0'\n",
    "    DATASET_ID     = 'gcp_bq'\n",
    "    TABLE_ID       = 'dados_engenharia_features'\n",
    "    FEATURESTORE_ID   = 'ecommerce_feature_store'\n",
    "    VALUES_ENTITY_ID = \"values\"\n",
    "    VALUES_BQ_SOURCE_URI = f\"bq://{PROJECT_ID},{DATASET_ID},{TABLE_ID}\"\n",
    "    FEATURE_TIME         = 'timestamp'\n",
    "    REGION               = \"us-centrall\"\n",
    "    \n",
    "    PROJECT_NUMBER          = os.environ[\"CLOUD_ML_PROJECT_ID\"]\n",
    "    aiplatform.init(project = project_number, location = REGION)\n",
    "\n",
    "    try:  \n",
    "        # Checks if there is already a featurestore\n",
    "        ecommerce_feature_store = aiplatform.featurestore(f\"{FEATURESTORE_ID}\")\n",
    "        logging.info(f\"\"\" A feature store {FEATURESTORE_ID} já existe.\"\"\")\n",
    "    except:\n",
    "        # Create a featurestore\n",
    "        logging.info(f\"\"\" Criando a feature store: {FEATURESTORE_ID}, \"\"\")\n",
    "        ecommerce_feature_store = aiplatform.featurestore.create(\n",
    "            featurestore_id = f\"{FEATURESTORE_ID}\",\n",
    "            online_store_fixed_rode_count = 1, \n",
    "            sync = True,)\n",
    "    try:\n",
    "        # get entity type, if it alredy exists\n",
    "        values_entity_type = ecommerce_feature_store.get_entity_type(entity_type_id = VALUES_ENTITY_ID)\n",
    "    \n",
    "    except: \n",
    "        # else, create entity type\n",
    "        values_entity_type = ecommerce_feature_stare.create_entity_type( \n",
    "            entity_type_id = VALUES_ENTITY_ID, description = \"Values Entity\", sync = True\n",
    "        )\n",
    "        \n",
    "    values_feature_configs = [\"gross_revenue\": { \"value_type\" : \"DOUBLE\", \n",
    "                                                 \"description\": \"Gross Revenue\",\n",
    "                                                \"labels\"      : \"passed\"},\n",
    "                              \"recency_days\": { \n",
    "                                  \"value_type\" : \"DOUBLE\", \n",
    "                                  \"description\": \"Recency Days\",\n",
    "                                  \"labels\": {\"status\": \"passed\"},\n",
    "                              },\n",
    "                              \n",
    "                              \"qty_product\"   : {\n",
    "                                  \"value_type\" : \"DOUBLE\", \n",
    "                                  \"description\": \"Quantity products\",\n",
    "                                  \"labels\": {\"status\": \"passed\"},\n",
    "                              }, \n",
    "                              \"frequency\": {\n",
    "                                  \"value_type\" : \"DOUBLE\",\n",
    "                                  \"description\": \"Quantity products\", \n",
    "                                  \"labels\"     : {\"status\": \"passed\"},\n",
    "                              }, \n",
    "                              \"qty_returns\" :{\n",
    "                                  \"value_type\" : \"DOUBLE\",\n",
    "                                  \"description\": \"Quantity returns\", \n",
    "                                  \"labels\"     : {\"status\": \"passed\"},\n",
    "                              }]\n",
    "                                  \n",
    "        values_fature_ids   = values_entity_type.batch_create_features(\n",
    "            feature_configs = values_feature_configs, sync = True\n",
    "        )\n",
    "        VALUES_FEATURES_IDS = [ feature.nane for feature in values_feature_ids.list_features()]\n",
    "        \n",
    "        logging.info(f\"\"\" Ingerindo os dados na feature store { FEATURESTORE_ID}, \"\"\")\n",
    "        values_entity_type.ingest_from_bq(\n",
    "            feature_ids     = VALUES_FEATURES_IDS, \n",
    "            feature_tine    = FEATURE_TIME, \n",
    "            bq_source_uri   = VALUES_BQ_SOURCE_URI, \n",
    "            entity_id_field = VALUES_ENTITY_ID, \n",
    "            disable_online_serving = True, \n",
    "            worker_count           = 2, \n",
    "            sync                   = True,\n",
    "        )\n",
    "        # ENOBLE API: https://console.developers.google.com/apis/api/cloudresourcemanager.googleapis.com/overview?project-343941956592%22\n",
    "        # http://alinpractive.com/gcp-mlops-vertex-ai-feature-store/\n",
    "        # https://medium.com/hacking-talent/vertexais-feature-store-for-dummies-3d798b45ece4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559e4e3a-6512-49e2-aa6b-4a297de8fd72",
   "metadata": {},
   "source": [
    "# 5. Componente: Utilizar feature store batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29df57ef-e30c-4136-a366-7ecf7e7de0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component( packages_to_install = [ \"google.cloud.aiplatform\",\n",
    "                                   \"google.cloud.bigquery\",\n",
    "                                   \"db-dtypes\", \n",
    "                                   \"pandas\"], \n",
    "          base_image = \"python:3.10.6\",\n",
    "          output_component_file = \"batch_serve_fs.yaml\")\n",
    "\n",
    "def create_batch_serve_fs():\n",
    "    import os\n",
    "    import logging \n",
    "    from typing import Union\n",
    "    \n",
    "    #from config import project_id\n",
    "    import pandas as pd\n",
    "    from google.cloud import aiplatform\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud.aiplatform import Feature, Featurestore\n",
    "    # https://medium.com/google-cloud/how-do-you-use-feature-store-in-the-mlops-process-on-vertex-ai-802ddca2cac4\n",
    "    # https://www.youtube.com/watch?v=jXD85fxAhvQ\n",
    "    \n",
    "    PROJECT_ID     = 'braided-period-442813-v0'\n",
    "    DATASET_ID     = 'gcp_bq'\n",
    "    REGION         = \"us-centrall\"\n",
    "    TABLE_INSTACES_ID = \"read_instances\"\n",
    "    TABLE_ID       = 'dados_engenharia_features'\n",
    "    SERVING_FEATURE_IDS ={ \"values\": [\"*\"]}\n",
    "    TABLE_TRAIN_ID = \"dados_treinamento\"\n",
    "    TRAIN_TABLE_URI = f\"bq://{PROJECT_ID}, {DATASET_ID},{TABLE_TRAIN_ID}\"\n",
    "    \n",
    "    FEATURE_STORE_NAME   = 'ecommerce_feature_store'\n",
    "    project_number = os.environ[\"CLOUD_ML_PROJECT_ID\"]\n",
    "    aiplatform.init(project = project_number, location = REGION)\n",
    "    \n",
    "     def run_bq_query(sql: str, project_name: str) -> Union[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Description\n",
    "            Run a BigQuery query and return the job ID or result as a DataFrame\n",
    "        Args:\n",
    "            sql: SQL query, as a string, to exercate in BigQuery\n",
    "        Returns:\n",
    "            df: DataFrame of results from query, or error, if any \n",
    "        \"\"\"\n",
    "        bq_client  = bigquery.Client(project = project_name)\n",
    "        \n",
    "        # Try dry run before executing query to catch any errors\n",
    "        job_config = bigquery.QueryJobConfig( dry_run = True, use_query_cache = False)\n",
    "        bq_client.query(sql, job_config = job_config)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # dry run succes without errors, ALGUMA to run query\n",
    "        job_config   = bigquery.QueryJobConfig()\n",
    "        client_result= bq_client.query(sql, job_config = job_config)\n",
    "        \n",
    "        job_id = client_result.job_id\n",
    "        \n",
    "        # wait for query /too to finish runnig get return data frame\n",
    "        df = client_result.result().to_arrow().to_pandas()\n",
    "        print(f\"Finished job id { job_id}\")\n",
    "        return df\n",
    "    \n",
    "    read_instaces_query = f\"\"\"\n",
    "                CREATE OR REPLACE TABLE  '{PROJECT_ID}, {DATASET_ID}, {TABLE_INTACES_ID}' as\n",
    "            (   SELECT\n",
    "                    values, \n",
    "                    timestamp, \n",
    "                FROM \n",
    "                    '{PROJECT_ID}, {DATASET_ID}, {TABLE_ID}');\n",
    "                \"\"\"\n",
    "    logging.info(\"Criando a tabela de instãncia\")\n",
    "        run_bq_query(read_instaces_query, project_name = PROJECT_ID)\n",
    "    \n",
    "    logging.info(f\"Iniciando o fornecimento das features da: {FEATURE_STORE_NAME}\")\n",
    "    ecommerce_feature_store = aiplatform.Featurestore( featurestore_name = FEATURE_STORE_NAME)\n",
    "    \n",
    "    logging.info(f\"Executando o comando para o destino:{TRAIN_TABLE_URI} a partir da tabela: {TABLE_INSTACES_ID}\")\n",
    "    ecommerce_feature_store.batch_serve_to_bq(\n",
    "        bq_destination_output_uri = TRAIN_TABLE_URI, \n",
    "        serving_feature_ids = SERVING_FEATURE_IDS, \n",
    "        read_instantes_uri = f\"bq://{PROJECT_ID},{DATASET_ID}, {TABLE_INSTACES_ID}\",\n",
    "    )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121ee79a-27be-4127-908b-49a56971f7e4",
   "metadata": {},
   "source": [
    "# 6. Componente: Treinamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd82107b-cec3-46c4-b716-5da19cc554f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component( packages_to_install = [ \"google.cloud.aiplatform\", \"pyarrow\", \"scikit-learn\",\n",
    "                                   \"google.cloud.bigquery\",\n",
    "                                   \"db-dtypes\", \n",
    "                                   \"pandas\"], \n",
    "          base_image = \"python:3.10.6\",\n",
    "          output_component_file = \"model_train.yaml\")\n",
    "def model_train( model: Output[Artifact]):\n",
    "    import os \n",
    "    import pickle\n",
    "    import pathlib\n",
    "    import logging\n",
    "    import datatime\n",
    "    from typing import Union\n",
    "    \n",
    "    import pandas as pd\n",
    "    \n",
    "    from google.cloud          import bigquery\n",
    "    from sklearn.cluster       import KMeans\n",
    "    from google.cloud          import aiplatform\n",
    "    from sklearn.pipeline      import Pipeline \n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    \n",
    "    logging.info(\"Iniciando o componente\")\n",
    "    \n",
    "    # VARIAVEIS \n",
    "    PROJECT_ID     = 'braided-period-442813-v0'\n",
    "    DATASET_ID     = 'gcp_bq'\n",
    "    REGION         = \"us-centrall\"\n",
    "    TABLE_TRAIN_ID = \"dados_treinamento\"\n",
    "    FEATURES       = ['qty_products', 'qty_returns', 'frequency', 'recency_days']\n",
    "    TARGET         = 'gross_revenue'\n",
    "    DEPLOY_VERSION = \"sklearn-cpu 1-0\"\n",
    "    FRAMEWORK      = \"scikit-learn\"\n",
    "    REGION_SPLITTED= \"us-centrall\".split(\"-\")[0]\n",
    "    DEPLOY_IMAGE   = f\"{REGION_SPLITTED}-docker.pkg.dev/vertex-ai/prediction/{DEPLOY_VERSION}:latest\"\n",
    "    N_CLUSTERS     = 8\n",
    "    scaler         = MinMaxScaler()\n",
    "    \n",
    "    project_number = os.environ[\"CLOUD_ML_PROJECT_ID\"]\n",
    "    aiplatform.init(project = project_numver, location = REGION)\n",
    "    \n",
    "    def run_bq_query(sql: str, project_name: str) -> Union[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Description\n",
    "            Run a BigQuery query and return the job ID or result as a DataFrame\n",
    "        Args:\n",
    "            sql: SQL query, as a string, to exercate in BigQuery\n",
    "        Returns:\n",
    "            df: DataFrame of results from query, or error, if any \n",
    "        \"\"\"\n",
    "        bq_client  = bigquery.Client(project = project_name)\n",
    "        \n",
    "        # Try dry run before executing query to catch any errors\n",
    "        job_config = bigquery.QueryJobConfig( dry_run = True, use_query_cache = False)\n",
    "        bq_client.query(sql, job_config = job_config)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # dry run succes without errors, ALGUMA to run query\n",
    "        job_config   = bigquery.QueryJobConfig()\n",
    "        client_result= bq_client.query(sql, job_config = job_config)\n",
    "        \n",
    "        job_id = client_result.job_id\n",
    "        \n",
    "        # wait for query /too to finish runnig get return data frame\n",
    "        df = client_result.result().to_arrow().to_pandas()\n",
    "        print(f\"Finished job id { job_id}\")\n",
    "        return df\n",
    "    \n",
    "    loggin.info(\"Carregando dados para o treinamento\")\n",
    "    df = run_bq_query(f'select * from {PROJECT_ID}, {DATASET_ID}, {TABLE_TRAIN_ID}', project_name = PROJECT_ID)\n",
    "    \n",
    "    logging.info( \"Iniciando o Treinamento\" )\n",
    "    \n",
    "    X = df[FEATURES].copy()\n",
    "    y = df[TARGET]\n",
    "    \n",
    "    model_pipeline = Pipeline(\n",
    "        [ \n",
    "            (\"pca\", PCA(n_components = 2 )), \n",
    "            (\"scaler\", scaler), \n",
    "            (\"clustering\", KMeans(n_clusters = N_CLUSTERS, random_state = 42)),\n",
    "        ]\n",
    "    )\n",
    "    model_pipeline.fit(X,y)\n",
    "    \n",
    "    logging.info(\"Criando o modelo output\")\n",
    "    model.metadata[\"framework\" ] = FRAMEWORK\n",
    "    model.metadata[\"containerSpec\"] = {\n",
    "        \"imageUri\": DEPLOY_IMAGE\n",
    "    }\n",
    "    file_name = model.path = f\"/{MODEL_NAME}\"\n",
    "    \n",
    "    pathlib.Path( model.path).mkdir()\n",
    "    with open(file_name, \"wb\" ) as file:\n",
    "        pickle.dump( model_pipeline, file) # Salvando o modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5170908e-ded7-4fc2-a464-b7e2338f59e8",
   "metadata": {},
   "source": [
    "# 7. Componente: Predição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec481b0-d000-4e5b-8a89-2b76d378adb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component( packages_to_install = [ \"google.cloud.aiplatform\", \"pyarrow\", \"scikit-learn\",\n",
    "                                   \"google.cloud.bigquery\", \"pandas\",\n",
    "                                   \"db-dtypes\", \n",
    "                                   \"pandas_gbq\"], \n",
    "          base_image = \"python:3.10.6\",\n",
    "          output_component_file = \"batch_prediction.yaml\")\n",
    "def batch_prediction( model: Input[Model]):\n",
    "    import os \n",
    "    import pickle\n",
    "    import pathlib\n",
    "    import logging\n",
    "    \n",
    "   \n",
    "    \n",
    "    import pandas_gbq\n",
    "    import pandas as pd\n",
    "    \n",
    "    from google.cloud          import bigquery\n",
    "    \n",
    "    from google.cloud          import aiplatform\n",
    "    from sklearn.pipeline      import Pipeline \n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    \n",
    "    # VARIAVEIS \n",
    "    PROJECT_ID     = 'braided-period-442813-v0'\n",
    "    DATASET_ID     = 'gcp_bq'\n",
    "    REGION         = \"us-centrall\"\n",
    "    TABLE_PREDICT_ID = \"dados_treinamento\"\n",
    "    MODEL_NAME     = \"model.pkl\"\n",
    "    TABLE_SAVE_PREDICTIONS_ID = \"dados_preditos\"\n",
    "    FEATURES       = ['qty_products', 'qty_returns', 'frequency', 'recency_days']\n",
    "    SQL_TO_PREDICT_DATA = f\"\"\" SELECT * \n",
    "                               FROM \"{PROJECT_ID},{DATASET_ID},{TABLE_TO_PREDICT_ID}\" \"\"\"\n",
    "    project_number = os.environ[\"CLOUD_ML_PROJECT_ID\"]\n",
    "    \n",
    "    aiplatform.init( project - project_number, location = REGION)\n",
    "    \n",
    "    logging.info( \"Iniciando o componete\")\n",
    "    \n",
    "    logging.info( f\" Carregando o modelo: {MODEL_NAME}\")\n",
    "    file_name = model.path + f\"/{MODEL_NAME}\"\n",
    "    with open( file_name, \"rb\") as file:\n",
    "        model_pipeline = pickle.load(file) \n",
    "    \n",
    "    logging.info(\"Carregando dados a serem preditos\")\n",
    "    predict_data = pd.read_gbq( SQL_TC_PREDICT_DATA, project_id = project_number)\n",
    "    \n",
    "    \n",
    "    logging.info(f\"Iniciando a predição dos dados: {PROJECT_ID}, {DATASET_ID}, {TABLE_TO_PREDICT_ID}\")\n",
    "    labels = model_pipeline.predict( predict_date[FEATURES])\n",
    "    predict_data[\"Clusters\"] = labels\n",
    "    \n",
    "    logging.info(f\"Substituindo os dados preditos: {PROJECT_ID}, {DATASET_ID}, {TABLE_SAVE_PREDICTIONS_ID}\")\n",
    "    pandas_gbq.to_gbq( predict_data,f\"{PROJECT_ID}, {DATASET_ID}, {TABLE_SABE_PREDICTIONS_ID}\", project_id = project_number,  if exists = 'replace')\n",
    "    labels = model\n",
    "   \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5f7288-491c-4324-a507-31978d117d65",
   "metadata": {},
   "source": [
    "# 8. PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14a7512-46e7-4684-8bb2-81d3cf222226",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_ROOT = \"gs://pipeline_ecommerce\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8dbf26-dd35-43a1-bc41-6b0d610b16c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline(pipeline_root = PIPELINE_ROOT, nane = \"ecommerce_clustering-pipeline\" )\n",
    "\n",
    "def ecommerce_clustering_pipeline():\n",
    "    # VARIAVEIS \n",
    "    PROJECT_ID     = 'braided-period-442813-v0'\n",
    "    DATASET_ID     = 'gcp_bq'\n",
    "    REGION         = \"us-centrall\"\n",
    "    MODEL_NAME     = \"ecommerce-clustering\"\n",
    "    \n",
    "    dataset_op     = get_data() \n",
    "    data_prep_op   = data_preparation()\n",
    "    data_prep_op.after(dataset_op)\n",
    "    \n",
    "    feature_engineering_op = feature_engineering()\n",
    "    feature_engineering_op.after(data_prep_op)\n",
    "    \n",
    "    feature_store_op = create_fearure_store()\n",
    "    feature_store_op.after( feature_store_op)\n",
    "    \n",
    "    batch_serve_fs_op = create_batch_serve_fs()\n",
    "    batch_serve_fs_op.after(batch_serve_fs_op)\n",
    "    \n",
    "    model_train_op = model_train()\n",
    "    model_train_op.after(batch_serve_fs_op)\n",
    "    \n",
    "    model_upload_op = gcc_aip.ModelUploadOp(\n",
    "        project = PROJECT)ID, \n",
    "        location = REGION, \n",
    "        display_name = f\"{MODEL_NAME}\",\n",
    "        unmanage_container_model = model_train_op.outputs[\"model\"], \n",
    "        ).after(model_train_op)\n",
    "    \n",
    "    batch_predict_op = batch_prediction( model = model_train_op.outputs[\"model\"])\n",
    "compiler.Compiler().compile( \n",
    "    pipeline_func =  ecommerce_clustering_pipeline, \n",
    "    package_path  = \"ecommerce_pipeline.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee4543f-e080-4169-833a-0fd159942acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = pipeline_jobs.PipelineJob(\n",
    "        display_name = \"ecommerce_pipeline\"\n",
    "        template_path= \"ecommerce_pipeline.json\",\n",
    "        enable_caching= False \n",
    "      )\n",
    "job.run(sync = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dff8e69-1e2e-4cb8-b2cf-dbf724c990ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud.aiplatform import Feature, Featurestore\n",
    "fs = Featurestore( \n",
    "    featurestore_name = \"ecommerce_feature_store\", \n",
    "    project = \"braided-period-442813-v0'\"\n",
    "    location= \"us-centrall\",\n",
    ")\n",
    "fs.delete(force = True"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
